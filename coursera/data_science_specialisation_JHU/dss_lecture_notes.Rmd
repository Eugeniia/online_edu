---
title: "Data Science specialization (Coursera)"
author: "Evgeniia Golovina"
date: "14/01/2021"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results="hide")

setwd("/Users/foffa/Desktop/Learning/online_edu")
```

This is an R Markdown document about my experience to learn R.

## The Data Scientist’s Toolbox (Coursera)

```{r The_Data_Scientists_Toolbox, echo=FALSE}
# installing from CRAN
install.packages("ggplot2")
install.packages(c("ggplot2", "devtools", "lme4"))

# installing from BioConductor
source("https://bioconductor.org/biocLite.R") # to get the basic functions required to install through BioConductor
biocLite("GenomicFeatures")

# installing from GitHub
install.packages("devtools")
library(devtools)
install_github("author/package")

library(ggplot2) # to load the package
installed.packages() # what packages are installed?
library() # what packages are installed?
old.packages() # what packages need an update?
update.packages() # to update all packages
install.packages("packagename")
detach("package:ggplot2", unload=TRUE) # to unload the package
remove.packages("ggplot2") # to remove the package

version() # to get R version
sessionInfo() # to get R version along with a listing of all of the packages you have loaded

help(package = "ggplot2")
browseVignettes("ggplot2") # to get extra help info
```

## R_Programming (Coursera)

```{r R Programming}
## Week 1
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/r_programming")
dir() # to list all the files in the working directory
ls() # to list all environment objects

empty_vector <- vector()
attributes() # to get attributes for R object
c() # to create vectors
x <- 9:29 # vector of integers
x <- c(1+0i, 2+4i) # vector of complex

x <- 1:6
class(x) # integer
as.numeric(x) # explicit coercion
as.logical(x) # explicit coercion
as.character(x) # explicit coercion

x <- list("a", 1, F)
m <- matrix(nrow = 2, ncol = 3); m # empty matrix
dim(m)
attributes(m)
m <- matrix(1:6, nrow = 2, ncol = 3); m # matrices are constructed column-wise
m <- 1:10; dim(m) <- c(2, 5); m # create a matrix from a vector by adding a dimension atrribute

x <- 1:3; y <- 10:12
cbind(x, y) # create a matrix by column-binding
rbind(x, y) # create a matrix by row-binding

x <- factor(c("yes", "no", "yes", "no", "no")) # create a factor with two levels: yes and no
table(x)
unclass(x)
x <- factor(c("yes", "no", "yes", "no", "no"),
            levels = c("yes", "no")) # to set the order of the levels

is.na() # to test object if they are NA
is.nan() # to test for NaN
x <- c(1, 2, NaN, NA, 4)
is.na(x); is.nan(x)

dta.frame(); read.table(); read.csv() # to create a data frame
data.matrix() # to convert dataframe to a matrix
x <- data.frame(foo = 1:4, bar = c(T, T, F, F)); x # foo and bar are column names
nrow(x); ncol(x)

x <- 1:3
names(x); names(x) <- c("a", "b", "c"); x; names(x)
x <- list(a = 1, b = 2, c = 3); names(x)
m <- matrix(1:4, nrow = 2, ncol = 2); dimnames(m) <- list(c("a", "b"), c("c", "d")); m

read.table(); read.csv() # to read tabular data
readLines() # ro read lines of a text file
source() # to read in R code files (inverse of dump())
dget() # to read in R code files (inverse of dput())
load() # to read in saved workspaces
unserialize() # to read single R objects in binary form

write.table()
writeLines()
dump()
dput()
save()
serialize()

y <- data.frame(a = 1, b = "c"); dput(y)
dput(y, file = "y.R"); new.y <- dget("y.R"); new.y # to save the object to  y.R file and read it later

x <- "foo"; y <- data.frame(a = 1, b = "a")
dump(c("x", "y"), file = "data.R"); rm(x, y); source("data.R")

# Connections interfaces to the outside world
file() # opens a connection to a file
gzfile() # opens a connection to a file compressed with gzip
bzfile() # opens a connection to a file compressed with bzip2
url() # opens a connection to a webpage

str(file)
con <- file("foo.txt", "r"); data <- read.csv(con); close(con)
data <- read.csv("foo.txt") # the same to previous line
con <- gzfile("foo.gz"); x <- readLines(con, 10) # con is usefult when reading part of the file

# Subsetting
# [ always returns a subset of the same class as original
# [[ used to subset elements of the list or data frame
# $ is used to extrcat elements of the list or data frame by name
x <- c("a", "b", "c"); x[1]; x[1:2]; x[x > "a"]
u <- x > "a"; u; x[u]

x <- list(foo = 1:4, bar = 0.6); x[1] # a list of [1, 2, 3, 4]
x[[1]] # a sequense 1,2,3,4
x$bar; x[["bar"]] # these two are the same
x["bar"] # returns a list with element 0.6 (bar)
x <- list(foo = 1:4, bar = 0.6, baz = "hello"); x[c(1, 3)] # to extract multimple elements of a list
# [[ can be used with computed indices
name <- "foo"; x[[name]]; x$name; x$foo # $ can use only literal names e.g. foo
x[[c(1,3)]]; x[[1]][[3]] # the third element in the first element of the list

x <- matrix(1:6, 2, 3); x[1,2] # first - row index, the second - column
x[1, ]; x[, 2]
x[1, 2] # returns a vector of length 1, by default it drops the dimension (e.g. matrix)
x[1, 2, drop = FALSE] # returns a matrix 1x1

# Partial matching of names is allowed with [[ nd $
x <- list(aardvar = 1:6); x$a
x[["a"]]; x[["a", exact = FALSE]]

# Removing NAs
x <- c(1, 2, NA, 3, NA, 4); bad <- is.na(x); bad; x[!bad]
y <- c("a", "b", NA, "c", NA)
good <- complete.cases(x, y); good; x[good]; y[good] # to subset from multiple things with NAs
airquality[1:6, ]
good <- complete.cases(airquality); airquality[good, ][1:6, ] # to remove all rows with NAs

# Vecorized operations
x <- 1:4; y <- 6:9
x + y; x > 2; y == 8; x * y

x <- matrix(1:4, 2, 2); y <- matrix(rep(10,4), 2, 2)
x * y # element-wise multiplication
x %*% y # true matrix multiplication

# swirl = statistics with interactive R learning
install.packages("swirl")
packageVersion("swirl") # ‘2.4.5’
library(swirl)
rm(list=ls()) # to slear the workspace
install_from_swirl("R Programming")
swirl() # to start swirl

## Week 2
# Control structures: if, else; for; while; repeat; break; next; return
# if, else
x <- 1
if(x > 3) {
  y <- 10
} else {
  y <- 0
}

y <- if(x > 3) {
  10
} else if(x == 0) {
  5
} else {
  0
}

# For loops
for(i in 1:10) {
  print(i)
}

x <- c("a", "b", "c", "d")
for(i in 1:4) {
  print(x[i])
}
for(i in seq_along(x)) {
  print(x[i])
}
for(letter in x) {
  print(letter)
}
for(i in 1:4) print(x[i]) # the compact style

# for loops can be nested, But! Nesting beyond 2-3 levels is very difficult to read/understand
x <- matrix(1:6, 2, 3)
for(i in seq_len(nrow(x))) {
  for(j in seq_len(ncol(x))) {
    print(x[i, j])
  }
}

# While loops
count <- 0
while(count < 5) {
  print(count)
  count <- count + 1
}

z <- 5
while(z >= 3 %% z <= 10) {
  print(z)
  coin <- rbinom(1, 1, 0.5)
  if(coin == 1) { # random walk
    z <- z + 1
  } else {
    z <- z - 1
  }
}

# repeat initiates an infinite loop; to ext the repeat loop use break!
x0 <- 1; tol <- 1e-8
repeat{
  x1 <- computeEstimate()
  if(abs(x1 - x0) < tol) {
    break
  } else {
    x0 <- x1
  }
}

# next structure
for(i in 1:100) {
  if(i <= 20) {
    next # skip the first 20 iterations
  } # do something
}

# R functions
add2 <- function(x, y) {
  x + y
}
add2(3, 5)

above10 <- function(x) {
  use <- x > 10
  x[use]
}

above <- function(x, n) {
  use <- x > n
  x[use]
}
x <- 1:20; above(x, 3)

columnMean <- function(x, removeNA = TRUE) {
  means <- numeric(ncol(x))
  for(i in 1:ncol(x)) {
    means[i] <- mean(x[, i], na.rm = removeNA)
  }
  means
}
x <- matrix(1:6, 2, 3); columnMean(x); columnMean(airquality); columnMean(airquality, FALSE)

formals(columnMean) # returns all the formal arguments of the fucntion
args(lm)

# lazy evaluation: the argumants are evaluated only as eeded
f <- function(a, b = 1, c = 2, d = NULL) {
  a*2
}
f(2)

myplot <- function(x, y, type = "1", ...) {
  plot(x, y, type = type, ...)
}

args(paste)
args(cat)

# Scoping rules
search() # to get the search list
f <- function(x, y) {
  x*2 + y/z # z is a free variable (not a formal argument, not a local variable)
}

make.power <- function(n){
  pow <- function(x){
    x^n
  }
  pow
}
cube <- make.power(3)
square <- make.power(2)
cube(3)
square(3)
ls(environment(cube)) # to check what's in the function's environment
get("n", environment(cube))

y <- 10
f <- function(x) {
  y <- 2
  y^2 + g(x)
}
g <- function(x) {
  x*y
}
f(3)

# Optimization routines: optim, nlm and optimize
# a constructor function - maximaxing the normal likelihood
make.NegLogLik <- function(data, fixed=c(FALSE, FALSE)){
  params <- fixed
  function(p){
    params[!fixed] <- p
    mu <- params[1]
    sigma <- params[2]
    a <- -0.5*length(data)*log(2*pi*sigma^2)
    b <- -0.5*sum((data-mu)^2)/(sigma^2)
    -(a+b)
  }
}
set.seed(1); normals <- rnorm(100,1,2)
nLL <- make.NegLogLik(normals); nLL
ls(environment(nLL))
optim(c(mu = 0, sigma = 1), nLL)$par
nLL <- make.NegLogLik(normals, c(FALSE, 2)) # fixing sigma to equal 2
optimize(nLL, c(-1, 3))$minimum # estimating parameters
nLL <- make.NegLogLik(normals, c(1, FALSE)) # fixing mu to equal 1
optimize(nLL, c(1e-6, 10))$minimum # estimating parameters
# ploting the likelihood
nLL <- make.NegLogLik(normals, c(1, FALSE)) # fixing mu to equal 1
x <- seq(1.7, 1.9, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = "l")
nLL <- make.NegLogLik(normals, c(FALSE, 2)) # fixing sigma to equal 2
x <- seq(0.5, 1.5, len = 100)
y <- sapply(x, nLL)
plot(x, exp(-(y - min(y))), type = "l")

# Dates and times
Date() # class for dates; stored internally as the number of datses since 1970-01-01
POSIXct(); POSIXlt() # class for times; staored internally as the number of seconds since 1970-01-01
x <- as.Date("1970-01-01"); x
unclass(x)
unclass(as.Date("1970-01-02"))
x <- Sys.time(); x # print current time
p <- as.POSIXlt(x); p
names(unclass(p)); p$sec
unclass(x); x$sec # Error in x$sec : $ operator is invalid for atomic vectors

datestring <- c("January 10, 2012 10:40", "December 9, 2011 11:40")
x <- strptime(datestring, "%B %d, %Y %H:%M"); x; class(x)
?strptime

x <- as.Date("2012-01-01")
y <- strptime("9 Jan 2011 11:34:45", "%d %b %Y %H:%M:%S"); y
x - y # Error in x - y : non-numeric argument to binary operator
x <- as.POSIXlt(x); x - y

x <- as.Date("2012-03-01"); y <- as.Date("2012-02-28"); x - y
x <- as.POSIXct("2012-10-25 01:00:00"); y <- as.POSIXct("2012-10-25 06:00:00", tz = "GMT"); y - x

# Quiz week 2
# Q1 - The number 27 is returned
cube <- function(x, n) {
        x^3
}
cube(3)
# Q2 - 'x' is a vector of length 10 and 'if' can only test a single logical statement.
x <- 1:10
if(x > 5) {
  x <- 0
}
# Q3 - 10
f <- function(x) {
        g <- function(y) {
                y + z
        }
        z <- 4
        x + g(x)
}
z <- 10
f(3)
# Q4 - 10
x <- 5
y <- if(x < 3) {
        NA
} else {
        10
}
# Q5 - f
h <- function(x, y = NULL, d = 3L) {
        z <- cbind(x, d)
        if(!is.null(y))
                z <- z + y
        else
                z <- z + f
        g <- x + y / z
        if(d == 3L)
                return(g)
        g <- g + 10
        g
}
# Q6 - a collection of symbol/value pairs
# Q7 - lexical scoping
# Q8 - The values of free variables are searched for in the environment in which the function was defined
# Q9 - All objects must be stored in memory
# Q10 - parent frame - It is the environment in which a function was called

# Programming Assignment 1
# Part 1
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/r_programming/w2_pa_1/")
pollutantmean <- function(directory, pollutant, id){
  files <- list.files(path=directory, pattern="*.csv", full.names=TRUE, recursive=FALSE)
  ourfiles <- files[id]
  rourfiles <- lapply(ourfiles, read.csv)
  t <- c()
  for(i in 1:length(rourfiles)){
    n <- rourfiles[[i]][[pollutant]]
    #print(length(n))
    t <- c(t, n)
    #print(length(t))
  }
  #print(length(t)) # 23739
  mean(t, na.rm = TRUE)
}

pollutantmean("specdata", "sulfate", 1:10) # 4.064128
pollutantmean("specdata", "nitrate", 70:72) # 1.706047
pollutantmean("specdata", "nitrate", 23) # 1.280833

directory <- "specdata"; pollutant <- "sulfate"; id <- 1:10
files <- list.files(path=directory, pattern="*.csv", full.names=TRUE, recursive=FALSE)
ourfiles <- files[id]; ourfiles
rourfiles <- lapply(ourfiles, read.csv); rourfiles
t <- c()
for(i in 1:length(rourfiles)){
    n <- rourfiles[[i]][[pollutant]]
    t <- c(t, n)
}; length(t)

# Part 2
directory <- "specdata"; id <- 1
files <- list.files(path=directory, pattern="*.csv", full.names=TRUE, recursive=FALSE)
ourfiles <- files[id]; ourfiles
rourfiles <- lapply(ourfiles, read.csv); rourfiles; class(rourfiles)
t <- c()
#a <- rourfiles[[id]]
#good <- complete.cases(a)
#new <- a[good, ]; n <- nrow(new); t <- c(t, n)
for(i in 1:length(rourfiles)){
  a <- rourfiles[[i]]
  good <- complete.cases(a)
  new <- a[good, ]
  n <- nrow(new)
  t <- c(t, n)
}
df <- data.frame(id = 1:length(rourfiles), nobs = t); df

complete <- function(directory, id){
  files <- list.files(path=directory, pattern="*.csv", full.names=TRUE, recursive=FALSE)
  ourfiles <- files[id]
  rourfiles <- lapply(ourfiles, read.csv)
  t <- c()
  for(i in 1:length(rourfiles)){
    a <- rourfiles[[i]]
    good <- complete.cases(a)
    new <- a[good, ]
    n <- nrow(new)
    t <- c(t, n)
  }
  data.frame(id = 1:length(rourfiles), nobs = t)
}

complete("specdata", 1)
complete("specdata", c(2, 4, 8, 10, 12))
complete("specdata", 30:25)
complete("specdata", 3)

# Part 3
directory <- "specdata"; threshold <- 5000
files <- list.files(path=directory, pattern="*.csv", full.names=TRUE, recursive=FALSE); length(files)
ccases <- complete("specdata", 1:length(files))
new_ccases <- ccases$nobs >= threshold
n <- ccases[new_ccases, ]
l <- dim(n)
correls <- c()
if(l[1] == 0){
  print("Data frame is empty!")
  correls <- numeric()
  correls
} else {
  print("Data frame is not empty!")
  v <- n[[1]]
  ourfiles <- files[v]
  rourfiles <- lapply(ourfiles, read.csv)
    for(i in 1:length(rourfiles)){
    a <- rourfiles[[i]]
    good <- complete.cases(a)
    new <- a[good, ]
    m <- data.matrix(data.frame(sulfate = new$sulfate, nitrate = new$nitrate))
    c <- cor(m)
    correls <- c(correls, c[1,2])
  }
}

?cor


corr <- function(directory, threshold = 0){
  files <- list.files(path=directory, pattern="*.csv", full.names=TRUE, recursive=FALSE)
  ccases <- complete(directory, 1:length(files))
  new_ccases <- ccases$nobs >= threshold
  n <- ccases[new_ccases, ]
  l <- dim(n)
  correls <- c()
  if(l[1] == 0) {
    print("Data frame is empty!")
    correls <- numeric()
    correls
  } else {
    v <- n[[1]]
    ourfiles <- files[v]
    rourfiles <- lapply(ourfiles, read.csv)
    for(i in 1:length(rourfiles)){
      a <- rourfiles[[i]]
      good <- complete.cases(a)
      new <- a[good, ]
      m <- data.matrix(data.frame(sulfate = new$sulfate, nitrate = new$nitrate))
      c <- cor(m)
      correls <- c(correls, c[1,2])
    }
    correls
  }
}

cr <- corr("specdata", 150)
head(cr)
summary(cr)

cr <- corr("specdata", 400)
head(cr)
summary(cr)

cr <- corr("specdata", 5000)
summary(cr)
length(cr)

cr <- corr("specdata")
summary(cr)
length(cr)

#PA1 Q1
round(pollutantmean("specdata", "sulfate", 1:10), 3) # 4.064
#PA1 Q2
round(pollutantmean("specdata", "nitrate", 70:72), 3) # 1.706
#PA1 Q3
round(pollutantmean("specdata", "sulfate", 34), 3) # 1.477
#PA1 Q4
round(pollutantmean("specdata", "nitrate"), 3) # 1.703
#PA1 Q5
cc <- complete("specdata", c(6, 10, 20, 34, 100, 200, 310))
print(cc$nobs)
#PA1 Q6 - 219
cc <- complete("specdata", 54)
print(cc$nobs)
#PA1 Q7 - 711 135  74 445 178  73  49   0 687 237
vstr <- "3.5.1"
RNGversion(vstr)
set.seed(42)
cc <- complete("specdata", 332:1)
use <- sample(332, 10)
print(cc[use, "nobs"])
#PA1 Q8 - [1]  0.2688  0.1127 -0.0085  0.4586  0.0447
vstr <- "3.5.1"
RNGversion(vstr)
cr <- corr("specdata")                
cr <- sort(cr)                
set.seed(868)                
out <- round(cr[sample(length(cr), 5)], 4)
print(out)
#PA1 Q9 - [1] 243.0000   0.2540   0.0504  -0.1462  -0.1680   0.5969
vstr <- "3.5.1"
RNGversion(vstr)
cr <- corr("specdata", 129)                
cr <- sort(cr)                
n <- length(cr)                
set.seed(197)                
out <- c(n, round(cr[sample(n, 5)], 4))
print(out)
#PA1 Q10 - [1]  0.0000 -0.0190  0.0419  0.1901
cr <- corr("specdata", 2000)                
n <- length(cr)                
cr <- corr("specdata", 1000)                
cr <- sort(cr)
print(c(n, round(cr, 4)))

## Week 3 - Loop functions and debugging
lapply() # to loop over a list and evaluate a function on each element
sapply() # -//-// but try to simplify the result
apply() # to apply function over the margins of an array
tapply() # to apply a function over subsets of a vector
mapply() # multivariate version of laaply

# lapply always returns a list
l <- list(a = 1:5, b = rnorm(10))
f <- function(x) x+1
lf <- lapply(l, f); lf
lmean <- lapply(l, mean); lmean # a list is returned
x <- 1:4; lapply(x, runif) # to generate uniform random variables using randon number generator: it generates one, two, three and four random variables
x <- 1:4; lapply(x, runif, min=0, max=10)

# Apply anonymous functions
x <- list(a = matrix(1:4, 2, 2), b = matrix(1:6, 3, 2)); x
lapply(x, function(elt) elt[,1]) # to extract the first column of each matrix

# sapply
l <- list(a = 1:5, b = rnorm(10))
smean <- sapply(l, mean); smean # a vector is returned

# apply
str(apply)
x <- matrix(rnorm(200), 20, 10); x # 20 rows and 10 columns
apply(x, 2, mean) # to calculate the mean of each colum (second dimension) in the matrix; vector
apply(x, 1, sum) # to calculate the summ of each row (first dimension) in the matrix; vector
#col/row sums and means general functions
rowSums = apply(x, 1, sum)
rowMeans = apply(x, 1, mean)
colSums = apply(x, 2, sum)
colMeans = apply(x, 2, mean)
# using the ... argument, passing the 25% and 75%
apply(x, 1, quantile, probs = c(0.25, 0.75))
# To average matrix in an array
a <- array(rnorm(2 * 2 * 10), c(2, 2, 10)); a
apply(a, c(1, 2), mean)
rowMeans(a, dims = 2)

# mapply
str(mapply)
x <- list(rep(1,4), rep(2, 3), rep(3, 2), rep(4, 1)); x
mapply(rep, 1:4, 4:1) # the same
# vectorizing a function
noise <- function(n, mean, sd){ # to generate random normal noise
  rnorm(n, mean, sd)
}
noise(5, 1, 2)
noise(1:5, 1:5, 2) # not really correct
mapply(noise, 1:5, 1:5, 2)
list(noise(1,1,2), noise(2,2,2), noise(3,3,2), noise(4,4,2), noise(5,5,2))

# tapply
str(tapply)
x <- c(rnorm(10), runif(10), rnorm(10, 1)); x
f <- gl(3, 10); f # craete a factor variable; three levels repeated 10 times
tapply(x, f, mean, simplify = FALSE) # return a list
tapply(x, f, mean, simplify = TRUE) # return a vector
tapply(x, f, range)

# split
str(split)
x <- c(rnorm(10), runif(10), rnorm(10, 1)); x
f <- gl(3, 10); f
spl <- split(x, f) # always returns a list back
lapply(spl, mean); sapply(spl, mean)

library(datasets)
head(airquality)
s <- split(airquality, airquality$Month)
lapply(s, function(x) colMeans(x[, c("Ozone", "Solar.R", "Wind")]))
sapply(s, function(x) colMeans(x[, c("Ozone", "Solar.R", "Wind")]))
sapply(s, function(x) colMeans(x[, c("Ozone", "Solar.R", "Wind")], na.rm = TRUE))
x <- rnorm(10); f1 <- gl(2, 5); f2 <- gl(5, 2); f1; f2; interaction(f1, f2)
str(split(x, list(f1, f2))) # there are empty levels
str(split(x, list(f1, f2), drop = TRUE)) # get rid of empty levels

# Debugging functions
str(traceback)
str(debug)
str(browser)
str(trace)
str(recover)

mean(k)
traceback() # call it right after the error occured
lm(k ~ x)
traceback()

debug(lm)
lm(k ~ x)

options(error = recover)
read.csv("nosuchfile")

# Quiz week 3
# Q1
library(datasets)
data(iris)
?iris
head(iris)
s <- split(iris, iris$Species)
sapply(s, function(x) colMeans(x[, c("Sepal.Length", "Sepal.Width")], na.rm = TRUE))
round(6.588, 0) # 7
# Q2
apply(iris[, 1:4], 2, mean)
# Q3
library(datasets)
data(mtcars)
head(mtcars)
?mtcars
tapply(mtcars$mpg, mtcars$cyl, mean) # yes
with(mtcars, tapply(mpg, cyl, mean)) # yes
sapply(split(mtcars$mpg, mtcars$cyl), mean) # yes
# Q4
s <- sapply(split(mtcars$hp, mtcars$cyl), mean); s
round(s[1] - s[3], 0) # absolute differenc ewill be 127
# Q5
debug(ls); ls()

#PA1 Q1
makeVector <- function(x = numeric()) {
        m <- NULL
        set <- function(y) {
                x <<- y
                m <<- NULL
        }
        get <- function() x
        setmean <- function(mean) m <<- mean
        getmean <- function() m
        list(set = set, get = get,
             setmean = setmean,
             getmean = getmean)
}

cachemean <- function(x, ...) {
        m <- x$getmean()
        if(!is.null(m)) {
                message("getting cached data")
                return(m)
        }
        data <- x$get()
        m <- mean(data, ...)
        x$setmean(m)
        m
}

test <- makeVector(c(2,4,6,8))
test <- makeVector(rnorm(100))
test$getmean()
# returns NULL because R discards the object when the function has finished
cachemean(test)
test$getmean()
cachemean(makeVector(1:6))


## Week 4 - Simulation and Profiling
# The str function
str(str) # an alternative to summary(); to compactly display the internal structure of an R object
x <- rnorm(100, 2, 4); summary(x); str(x)

# Generating random numbers
rnorm(100, 2, 4)
str(dnorm); str(pnorm); str(rpois)
str(dnorm); str(pnorm); str(qnorm); str(rnorm) # d - density; r - random number generation; p cumulative distribution; q - quantile function
set.seed(1) # set the random number seed to ensure reproducibility

# Simulating a linear model
set.seed(20)
x <- rnorm(100); e <- rnorm(100, 0, 2)
y <- 0.5 + 2 * x + e; summary(y); plot(x,y)
# what if a is binary?
set.seed(10)
x <- rbinom(100, 1, 0.5) # generate binomary distribution
e <- rnorm(100, 0, 2)
y <- 0.5 + 2 * x + e; summary(y); plot(x,y)
# Generating random numbers from a Generalized Linear Model
set.seed(1)
x <- rnorm(100); log.mu <- 0.5 + 0.3 * x
y <- rpois(100, exp(log.mu)); summary(y); plot(x,y) # a Poisson model

# Random sampling
str(sample) # randomly sample from arbitrary distributions
set.seed(1)
sample(1:10, 4)
sample(letters, 5)
sample(1:10) # permutation
sample(1:10, replace = TRUE) # sample with replacement

# R profiler
str(system.time) # to compute the time needed to execute the expression
system.time(ls)
# Rprof
str(Rprof)
summaryRprof() # Don't use system.time() and Rprof together!!!

# Quiz week 4
# Q1 - [1] 1 1 2 4 1
set.seed(1)
rpois(5, 2)
# Q2 - rnorm
# Q3 - It ensures that the sequence of random numbers starts in a specific place and is therefore reproducible. It can be used to specify which random number generating algorithm R should use, ensuring consistency and reproducibility.
# Q4 - qpois
# Q5 - Generate data from a Normal linear model
set.seed(10)
x <- rep(0:1, each = 5)
e <- rnorm(10, 0, 20)
y <- 0.5 + 2 * x + e
# Q6 - rbinom
# Q7 - the function call stack
# Q8 - 100%
library(datasets)
Rprof()
fit <- lm(y ~ x1 + x2)
Rprof(NULL)
# Q9 - It is the time spent by the CPU evaluating an expression
# Q10 - elapsed time may be smaller than user time

# PA 3
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/r_programming/w4_pa3/hospital_quality")
outcome <- read.csv("outcome-of-care-measures.csv", colClasses = "character")
head(outcome)
ncol(outcome); nrow(outcome); names(outcome)
# Part 1 - Plot the 30-day mortality rates for heart attack
outcome[, 11] <- as.numeric(outcome[, 11])
hist(outcome[, 11])
# Part 2 - Finding the best hospital in a state
out <- read.csv("outcome-of-care-measures.csv")
state <- "TX"; outcome <- "heart attack"
states <- unique(out$State); states
outcomes <- c("heart attack", "heart failure", "pneumonia"); outcomes
if((state %in% states) && (outcome %in% outcomes)){
  ## Return hospital name in that state with lowest 30-day death rate
  new <- out[out$State == state, ]
  if(outcome == "heart attack") {
    ha <- new[ ,c(2, 13)]
    ha[, 2] <- as.numeric(ha[, 2])
    minha <- min(ha[, 2], na.rm = TRUE) # 8
    ha[1][which(ha[, 2] == minha), ]
  } else if(outcome == "heart failure"){
    ha <- new[ ,c(2, 19)]
    ha[, 2] <- as.numeric(ha[, 2])
    minha <- min(ha[, 2], na.rm = TRUE) # 5.2
    ha[1][which(ha[, 2] == minha), ]
  } else if(outcome == "pneumonia"){
    ha <- new[ ,c(2, 25)]
    ha[, 2] <- as.numeric(ha[, 2])
    minha <- min(ha[, 2], na.rm = TRUE)
    ha[1][which(ha[, 2] == minha), ]
  }
} else if(!is.element(state, states)) {
  stop("Error: invalid state")
} else if(!is.element(outcome, outcomes)) {
  stop("Error: invalid outcome")
}

best <- function(state, outcome) {
  ## Read outcome data
  out <- read.csv("outcome-of-care-measures.csv", colClasses = "character")
  ## Check that state and outcome are valid
  states <- unique(out$State)
  outcomes <- c("heart attack", "heart failure", "pneumonia")
  if((state %in% states) && (outcome %in% outcomes)) {
    new <- out[out$State == state, ]
    if(outcome == "heart attack") {
      ha <- new[ ,c(2, 13)]
      ha[, 2] <- as.numeric(ha[, 2])
      minha <- min(ha[, 2], na.rm = TRUE) # 8
      ha[1][which(ha[, 2] == minha), ]
    } else if(outcome == "heart failure"){
      ha <- new[ ,c(2, 19)]
      ha[, 2] <- as.numeric(ha[, 2])
      minha <- min(ha[, 2], na.rm = TRUE) # 5.2
      ha[1][which(ha[, 2] == minha), ]
    } else if(outcome == "pneumonia"){
      ha <- new[ ,c(2, 25)]
      ha[, 2] <- as.numeric(ha[, 2])
      minha <- min(ha[, 2], na.rm = TRUE)
      ha[1][which(ha[, 2] == minha), ]
    }
  } else if(!is.element(state, states)) {
    stop("invalid state")
  } else if(!is.element(outcome, outcomes)) {
    stop("invalid outcome")
  }
}

# Testing
best("TX", "heart attack")
best("TX", "heart failure")
best("MD", "heart attack")
best("MD", "pneumonia")
best("BB", "heart attack")
best("NY", "hert attack")

# Part 3 - Ranking hospitals by outcome in a state
state <- "WI"; outcome <- "pneumonia"; num <- 3
out <- read.csv("outcome-of-care-measures.csv", colClasses = "character")
new <- out[out$State == state, ]
#ne <- new[, c(2, 23)]
#ne[, 2] <- as.numeric(new[, 2])
#new[, 11] <- as.numeric(new[, 11])
#new[, 17] <- as.numeric(new[, 17])
#new[, 23] <- as.numeric(new[, 23])
n <- new[, c(2, 11, 17, 23)]
n[, 2] <- as.numeric(n[, 2])
n[, 3] <- as.numeric(n[, 3])
n[, 4] <- as.numeric(n[, 4])
p <- n[ ,c(1, 4)]
good <- complete.cases(p)
new <- p[good, ]
new$rank <- NA
num
#ha <- new[ ,c(1, 4, 5)]
#ha[, 2] <- as.numeric(ha[, 2])
#d <- ha[order(ha[,2], ha[,1]),]
ordered_scores <- order(ha[,2], ha[,1])
#ordered_scores <- order(ha[, 2])
ha$rank[ordered_scores] <- 1:nrow(ha)
ha[1][which(ha[, 3] == num), ]


rankhospital <- function(state, outcome, num = "best") {
  ## Read outcome data
  out <- read.csv("outcome-of-care-measures.csv", colClasses = "character")
  ## Check that state and outcome are valid
  states <- unique(out$State)
  outcomes <- c("heart attack", "heart failure", "pneumonia")
  if((state %in% states) && (outcome %in% outcomes)){
    ## Return hospital name in that state with the given rank 30-day death rate
    new <- out[out$State == state, ]
    n <- new[, c(2, 11, 17, 23)]
    n[, 2] <- as.numeric(n[, 2])
    n[, 3] <- as.numeric(n[, 3])
    n[, 4] <- as.numeric(n[, 4])
    if(outcome == "heart attack"){
      ha <- n[ ,c(1, 2)]
      good <- complete.cases(ha)
      new <- ha[good, ]
      new$rank <- NA
      ordered_scores <- order(new[,2], new[,1])
      new$rank[ordered_scores] <- 1:nrow(new)
      if(num == "best"){
        new[1][which(new[, 3] == 1), ]
      } else if(num =="worst"){
        new[1][which(new[, 3] == tail(nrow(new), n=1)), ]
      } else if(num > nrow(new)){
        return(NA)
      } else{
        new[1][which(new[, 3] == num), ]
      }
    } else if(outcome == "heart failure"){
      ha <- n[ ,c(1, 3)]
      good <- complete.cases(ha)
      new <- ha[good, ]
      new$rank <- NA
      ordered_scores <- order(new[,2], new[,1])
      new$rank[ordered_scores] <- 1:nrow(new)
      if(num == "best"){
        new[1][which(new[, 3] == 1), ]
      } else if(num =="worst"){
        new[1][which(new[, 3] == tail(nrow(new), n=1)), ]
      } else if(num > nrow(new)){
        return(NA)
      } else {
        new[1][which(new[, 3] == num), ]
      }
    } else if(outcome == "pneumonia"){
      ha <- n[ ,c(1, 4)]
      good <- complete.cases(ha)
      new <- ha[good, ]
      new$rank <- NA
      ordered_scores <- order(new[,2], new[,1])
      new$rank[ordered_scores] <- 1:nrow(new)
      if(num == "best"){
        new[1][which(new[, 3] == 1), ]
      } else if(num =="worst"){
        new[1][which(new[, 3] == tail(nrow(new), n=1)), ]
      } else if(num > nrow(new)){
        return(NA)
      } else {
        new[1][which(new[, 3] == num), ]
      }
    }
  } else if(!is.element(state, states)){
    stop("invalid state")
  } else if(!is.element(outcome, outcomes)){
    stop("invalid outcome")
  }
}

# Testing
rankhospital("TX", "heart failure", 4)
rankhospital("MD", "heart attack", "worst")
rankhospital("MN", "heart attack", 5000)
rankhospital("AL", "heart attack", 20) #  D W MCMILLAN MEMORIAL HOSPITAL
rankhospital("WI", "pneumonia", "worst") #  MAYO CLINIC HEALTH SYSTEM - NORTHLAND, INC

# Part 4 - Ranking hospitals in all states
library(tidyverse)
out <- read.csv("outcome-of-care-measures.csv")
outcome <- "heart attack"; num <- 20
states <- unique(out$State)
for(state in 1:length(states)){
  rankhospital(state, outcome, num)
}


rankall <- function(outcome, num = "best") {
  ## Read outcome data
  out <- read.csv("outcome-of-care-measures.csv")
  ## Check that state and outcome are valid
  states <- unique(out$State)
  outcomes <- c("heart attack", "heart failure", "pneumonia")
  df <- data.frame(hospital=c(), state=c())
  if(outcome %in% outcomes){
    ## For each state, find the hospital of the given rank
    for(i in 1:length(states)){
      o <- rankhospital(states[i], outcome, num)
      l <- list(hospital=o, state=states[i])
      df <- rbind(df, l)
      #print(class(l))
    }
  ## Return a data frame with the hospital names and the (abbreviated) state name
  df[order(df[,2]),]
  } else if(!is.element(outcome, outcomes)){
    stop("invalid outcome")
  }
}

# Testing
head(rankall("heart attack", 20), 10)
tail(rankall("pneumonia", "worst"), 3)
tail(rankall("heart failure"), 10)

#PA3 Q1
best("SC", "heart attack") # "MUSC MEDICAL CENTER"
# PA3 Q2
best("NY", "pneumonia") # "MAIMONIDES MEDICAL CENTER"
# PA3 Q3
best("AK", "pneumonia") # "YUKON KUSKOKWIM DELTA REG HOSPITAL"
# PA3 Q4
rankhospital("NC", "heart attack", "worst") # "WAYNE MEMORIAL HOSPITAL"
# PA3 Q5
rankhospital("WA", "heart attack", 7) # "YAKIMA VALLEY MEMORIAL HOSPITAL"
# PA3 Q6
rankhospital("TX", "pneumonia", 10) # "SETON SMITHVILLE REGIONAL HOSPITAL"
# PA3 Q7
rankhospital("NY", "heart attack", 7) # "BELLEVUE HOSPITAL CENTER"
# PA3 Q8
r <- rankall("heart attack", 4)
as.character(subset(r, state == "HI")$hospital) # "CASTLE MEDICAL CENTER"
# PA3 Q9
r <- rankall("pneumonia", "worst")
as.character(subset(r, state == "NJ")$hospital) # "BERGEN REGIONAL MEDICAL CENTER"
# PA3 Q10
r <- rankall("heart failure", 10)
as.character(subset(r, state == "NV")$hospital) # "RENOWN SOUTH MEADOWS MEDICAL CENTER"
```

## Getting and Cleaning Data (Coursera)

```{r Getting and Cleaning Data, echo=FALSE}
## Week 1
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/getting_and_cleaning_data")
file.exists("data") # check if the dir exists
dir.create("data") # create the data dir
if(!file.exists("data")){
  dir.create("data")
}
download.file() # get data from the internet

fileURL <- "https://www.smth"
download.file(fileURL, destfile = "./data/cameras.csv", method = "curl") # curl for https
list.files("./data")
dateDownloaded <- date(); dateDownloaded

library(xlsx) # to read xlsx files, XLConnect package
data <- read.xlsx("./data.xlsx", sheetIndex=1, header=TRUE); head(data)
data <- read.xlsx2("./data.xlsx", sheetIndex=1, header=TRUE); head(data) # much faster
data <- read.xlsx("./data.xlsx", sheetIndex=1, header=TRUE, colIndex=2:3, rowIndex=1:3)
write.xlsx()

# reading xml files
library(XML)
fileURL <- "http://smth"
doc <- xmlTreeParse(fileURL, useInternalNodes = TRUE) # htmlTreeParse for html files
rootNode <- xmlRoot(doc)
xmlName(rootNode); names(rootNode)
rootNode[[1]]; rootNode[[1]][[1]]
xmlSApply(rootNode, xmlValue) # programatically extract parts of the file
# XPath language
xpathSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//price", xmlValue)

# reading json
library(jsonlite)
jsonData <- fromJSON("https://api.smth") # from API
names(jsonData); names(jsonData$owner)
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
iris2 <- fromJSON(myjson); head(iris2)

# data.table() 
library(data.table) # much faster than data.frame
df <- data.frame(x=rnorm(9), y=rep(c("a", "b", "c"), each=3), z=rnorm(9)); head(df, 3)
dt <- data.table(x=rnorm(9), y=rep(c("a", "b", "c"), each=3), z=rnorm(9)); head(df, 3)
tables() # see all the data tables in memory
dt[2,]; dt[dt$y=="a",]
dt[c(2,3)] # subsetting rows
dt[,c(2,3)] # subsetting columns?!? Nope
# expression - a collections of statements
{
  x = 1
  y = 2
}
k = {print(10);5} # 10
print(k) # 5
dt[, list(mean(x), sum(z))]
dt[,table(y)] # get y as a table
dt[,w:=z^2] # adding new columns
dt2 <- dt; dt[, y:=2]; head(dt, 3); head(dt2, 3) # use a copy function to copy the data table!
dt[, m:= {tmp <- (x+z); log2(tmp+5)}] # multiple operations
dt[, a:=x>0] # plyr like operations
dt[, b:= mean(x+w),by=a] # group by a

set.seed(123)
dt <- data.table(x=sample(letters[1:3], 1E5, TRUE))
dt[, .N, by=x] # special variables: .N - an integer of length 1 containing the number

dt <- data.table(x=rep(c("a", "b", "c"), each=100), y=rnorm(300))
setKey(dt, x); dt['a'] # Keys! to sort, subset much faster!
dt1 <- data.table(x=c("a", "b", "c", "dt1"), y = 1:4)
dt2 <- data.table(x=c("a", "b", "dt2"), z = 5:7)
setKey(dt1, x); setKey(dt2, x); merge(dt1, dt2) # the same key is y to mergy by x

big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names = FALSE, col.names = TRUE, sep = "\t", quote=FALSE)
system.time(fread(file))
system.time(read.table(file, header = TRUE, sep = "\t")) # slower

# Quiz week 1
# Q1 - 53
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/getting_and_cleaning_data")
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileURL, destfile = "./w1/data.csv", method = "curl")
dateDownloaded <- date(); dateDownloaded
data <- read.csv("./w1/data.csv"); head(data,3) # data$VAL == 24 to get property value >= $1,000,000
d <- data[data$VAL==24,]
t <- data$VAL; bad <- is.na(t); bad; m <- t[!bad]; length(m[m == 24])
# Q2 - Tidy data has one variable per column.
fesdf <- data[, c("FES", "VAL")]; data$FES
# Q3 - 36534720
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileURL, destfile = "./w1/data.xlsx", method = "curl")
dateDownloaded <- date(); dateDownloaded
install.packages("xlsx"); library(xlsx)
dat <- read.xlsx("./w1/data.xlsx", sheetIndex=1, header=TRUE, colIndex=7:15, rowIndex=18:23); dat
sum(dat$Zip*dat$Ext,na.rm=T) # 36534720
# Q4 - 127
install.packages("XML"); library(XML)
#fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse("./w1/getdata_data_restaurants.xml", useInternalNodes = TRUE)
rootNode <- xmlRoot(doc); zipcodes <- xpathSApply(rootNode, "//zipcode", xmlValue);
length(zipcodes[zipcodes=="21231"])
# Q5 - 
install.packages("data.table"); library(data.table)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w1/q5.csv", method = "curl")
dateDownloaded <- date(); dateDownloaded
DT <- fread("./w1/q5.csv"); v <- DT$pwgtp15; v
system.time(mean(DT[DT$SEX==1,]$pwgtp15)); system.time(mean(DT[DT$SEX==2,]$pwgtp15))
system.time(tapply(DT$pwgtp15,DT$SEX,mean)) # nope
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean)) # nope
system.time(rowMeans(DT)[DT$SEX==1]); system.time(rowMeans(DT)[DT$SEX==2])
system.time(mean(DT$pwgtp15,by=DT$SEX)) 
system.time(DT[,mean(pwgtp15),by=SEX]) # yes

## Week 2
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/getting_and_cleaning_data/w2")
# Reading mySQL
install.packages("RMySQL"); library(RMySQL)
# to install mysql:
# cd /Library/LaunchDaemons
# sudo launchctl load -F com.oracle.oss.mysql.mysqld.plist # pass the macbook password
# About mysql shell - https://dev.mysql.com/doc/mysql-shell/8.0/en/mysql-shell-install.html
# https://genome.ucsc.edu/goldenPath/help/mysql.html
ucscDB <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDB, "show databases;"); result; dbDisconnect(ucscDB)

hg19 <- dbConnect(MySQL(), user="genome", db="hg19", host="genome-mysql.cse.ucsc.edu")
all.tables <- dbListTables(hg19); length(all.tables); all.tables[1:5]
dbListFields(hg19, "affyU133Plus2") # get column names of the table
dbGetQuery(hg19, "select count(*) from affyU133Plus2") # get number of rows in the table
affyData <- dbReadTable(hg19, "affyU133Plus2"); head(affyData) # get as a dataframe
# select specific subset
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
affyMisSmall <- fetch(query, n=10); dbClearResult(query); dim(affyMisSmall)
dbDisconnect(hg19) # remember to close the connection!

# Reading from HDF5
#source("http://bioconductor.org/biocLite.R"); biocLite("rhdf5")
install.packages("BiocManager"); library(BiocManager); update.packages("BiocManager")
BiocManager::install(version = "3.12")
BiocManager::install(c("GenomicFeatures", "AnnotationDbi")) # example
BiocManager::install("rhdf5"); library(rhdf5)
created <- h5createFile("example.h5"); created
# create groups
created <- h5createGroup("example.h5", "foo")
created <- h5createGroup("example.h5", "baa")
created <- h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
# write to groups
A <- matrix(1:10, nr=5, nc=2); h5write(A, "example.h5", "foo/A")
B <- array(seq(0.1, 2.0, by=0.1), dim=c(5,2,2))
attr(B, "scale") <- "liter"; h5write(B, "example.h5", "foo/foobaa/B")
h5ls("example.h5")
# write a data set
df <- data.frame(1L:5L,seq(0,1,length.out = 5),
                 c("ab", "cde", "fdh", "a", "s"), stringsAsFactors = FALSE)
h5write(df, "example.h5", "df"); h5ls("example.h5")
# reading data
readA <- h5read("example.h5", "foo/A"); readA
readB <- h5read("example.h5", "foo/foobaa/B"); readB
readdf <- h5read("example.h5", "df"); readdf
# writing and reading chunks
h5write(c(12,13,14), "example.h5", "foo/A", index=list(1:3,1)) # writing 12, 13, 14 to the first three rows of the first column in foo/A
h5read("example.h5", "foo/A")

# Reading from the Web
con <- url("https://scholar.google.ru/citations?user=cR7t9_8AAAAJ&hl=en")
htmlCode <- readLines(con)
close(con); htmlCode

library(XML)
url <- "https://scholar.google.ru/citations?user=cR7t9_8AAAAJ&hl=en"
html <- xmlTreeParse(url, useInternalNodes = TRUE)
xpathSApply(html, "//title", xmlValue)

install.packages("httr"); library(httr)
html2 <- GET(url)
content2 <- content(html2, as="text")
parsedHTML <- htmlParse(content2, asText = T)
xpathSApply(parsedHTML, "//title", xmlValue)

pg2 <- GET("the url to password page", authenticate("user", "passswd")) # give username and password
pg2; names(pg2)
# using handles
google <- handle("https://www.google.ru")
pg1 <- GET(handle = google, path="/")
pg2 <- GET(handle = google, path="search")

# Reading from APIs
# accessing twitter from R
library(httr)
myapp <- oauth_app("twitter", key = "yourConsumerKeyHere", secret="yourConsumerSecretHere") # start the autorization process
sig <- sign_oauth1.0(myapp, token="yourTokenHere", token_secret = "yourTokenSecretHere")  # to sign in
homeTL <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig) # 1.1 is a version of the API; statuses - which data I'd like to get out
# converting the json object
json1 <- content(homeTL) # to extract the json data
json2 <- jsonlite::fromJSON(toJSON(json1)) # jsonlite package to reformat json as datafame
json2[1, 1:4]

# Reading from other sources
install.packages("foreign"); library(foreign) # read.foo where foo is a file extension
read.arff() # to read Weka
read.dta() # Stata
read.mtp() # Minitab
read.octave() # Octave
read.spss() # SPSS
read.xport() # SAS
# RPostresSQL, RODBC, RMongo, rmongodb packages
# reading images: jpeg, readbitmap, png and EBImage packages
# reading GIS data: rdgal, rgeos, raster packages
# reading music data: tuneR, seewave packages

# Week 2 Quiz
# Q1 - 2013-11-07T13:25:07Z
install.packages("httpuv")
library(httr); library(httpuv)
oauth_endpoints("github")
myapp <- oauth_app("github", key = "987215e7eb60495356dc",
                   secret = "41b8888ac93af0fe65805862c116b2d75cb8027b") # to create a new OAuth app --> https://github.com/settings/developers, the URL is http://github.com and the Authorization callback URL is http://localhost:1410, specify key and secret
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp) # Get OAuth credentials
gtoken <- config(token = github_token) # Use API
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
cont <- content(req)
t <- c()
cont[[1]][["full_name"]]
for(i in 1:length(cont)){
  t <- c(t, cont[[i]][["full_name"]])
}
cont[[23]][["created_at"]]
# Q2 - sqldf("select pwgtp1 from acs where AGEP < 50")
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/getting_and_cleaning_data")
install.packages("sqldf"); library(sqldf)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w2/Q2.csv", method = "curl")
acs <- read.csv("./w2/Q2.csv"); head(acs,3)
sqldf("select * from acs") # nope
sqldf("select pwgtp1 from acs where AGEP < 50") # correct
sqldf("select pwgtp1 from acs") # nope
sqldf("select * from acs where AGEP < 50") # nope
# Q3 - sqldf("select distinct AGEP from acs")
unique(acs$AGEP)
sqldf("select unique * from acs") # nope, error
sqldf("select distinct pwgtp1 from acs") # nope
sqldf("select AGEP where unique from acs") # nope, error
sqldf("select distinct AGEP from acs") # yes
# Q4 - 45 31 7 25
library(XML); library(httr); library(httpuv)
url <- "http://biostat.jhsph.edu/~jleek/contact.html"
html <- GET(url)
content <- content(html, as="text")
parsedHTML <- htmlParse(content, asText = T)
# check the source code for the page
nchar('<meta name="Distribution" content="Global" />"') # 46 - 1
nchar('<script type="text/javascript">') # 31
nchar('  })();') # 7
nchar('				<ul class="sidemenu">') # 25

install.packages("RCurl"); library(RCurl)
surl <- "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
turl <- getURL(surl)
html <- htmlTreeParse(turl, useInternalNodes=T)
xpathSApply(html, "//td[@class='gsc_a_c']", xmlValue)
# Q5 - 32426.7
dd<-read.fwf("https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for", widths=c(10, rep(c(9,4),4)), skip=4)
sum(dd$V4) # 32426.7

## Week 3
# Subsetting and sorting
set.seed(13435)
x <- data.frame("var1"=sample(1:5), "var2"=sample(6:10), "var3"=sample(11:15))
x <- x[sample(1:5),]; x; x$var2[c(1,3)] = NA; x
x[,1]; x[,"var1"]; x[1:2,"var2"]
x[(x$var1 <= 3 & x$var3 > 11),]; x[(x$var1 <= 3 | x$var3 > 15),]
x[which(x$var2 > 8),] # dealing with missing values
# sorting
sort(x$var1); sort(x$var1, decreasing = T); sort(x$var2, na.last = T) # add NAs in the end of the sort
# ordering
x[order(x$var1),] # order by var1
x[order(x$var1, x$var3),] # order by var1 abd then by var3
library(plyr); arrange(x, var1); arrange(x, desc(var1)) # ordering with plyr
x$var4 <- rnorm(5); x # adding rows and columns
y <- cbind(x, rnorm(5)); y; y <- cbind(rnorm(5), x); y # column-binding
z <- rbind(x, rnorm(4)); z # row-binding

# Summarizing data
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/getting_and_cleaning_data")
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w2/Q2.csv", method = "curl")
acs <- read.csv("./w2/Q2.csv"); head(acs,3); tail(acs,4) # default is 6
summary(acs)
str(acs)
quantile(acs$pwgtp80, na.rm = T); quantile(acs$pwgtp80, na.rm = T, probs = c(0.5, 0.75, 0.9))
table(acs$pwgtp80, useNA = "ifany") # make a table
table(acs$pwgtp80, acs$pwgtp79) # make a 2-dimensional table
sum(is.na(acs$pwgtp80)) # check for missing values
any(is.na(acs$pwgtp80)) # FALSE
all(acs$pwgtp80 > 0) # TRUE - all have values more than 0
colSums(is.na(acs)); rowSums(is.na(acs))
all(colSums(is.na(acs))==0) # FALSE
table(acs$SERIALNO %in% c(85383)); table(acs$SERIALNO %in% c(85383, 91107)) # values with specific characteristics
acs[acs$SERIALNO %in% c(85383, 91107),] # to get a subset with specific values
# cross tabs
data("UCBAdmissions")
df <- as.data.frame(UCBAdmissions); summary(df)
xt <- xtabs(Freq ~ Gender + Admit, data=df); xt # get Freq info for Geneder across Admit
# flat tables
warpbreaks$replicate <- rep(1:9, len=54)
xt <- xtabs(breaks ~ ., data=warpbreaks); xt # . means break it down across all variables
ftable(xt) # to make a flat table
# size of the dataset
fakedata <- rnorm(1e5); object.size(fakedata)
print(object.size(fakedata), units="Mb")

# Creating new variables
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/getting_and_cleaning_data")
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w2/Q2.csv", method = "curl")
acs <- read.csv("./w2/Q2.csv"); head(acs,3)
# creating sequences - to index your data set
s1 <- seq(1,10, by=2); s1 # increasing by 2
s2 <- seq(1,10, length=3); s2 # create exact three values
x <- c(1, 3, 8, 25, 100); seq(along=x) # create an index for x
acs$new <- acs$SERIALNO %in% c(85383, 91107); table(acs$new)
acs$newpw <- ifelse(acs$pwgtp80 > 20, TRUE, FALSE); table(acs$newpw, acs$pwgtp80 > 20) # binary vars
acs$newgr <- cut(acs$pwgtp80, breaks = quantile(acs$pwgtp80)); table(acs$newgr) # categorical vars
table(acs$newgr, acs$pwgtp80)
# easier cutting
install.packages("Hmisc"); library(Hmisc)
acs$newgr2 <- cut2(acs$pwgtp80, g=4); table(acs$newgr) # break into four groups according to quantiles
acs$zcf <- factor(acs$SERIALNO); acs$zcf[1:10]; class(acs$zcf) # create factor vars
# levels of factor variables
yesno <- sample(c("yes", "no"), size=10, replace=T); yesno
yesnofac <- factor(yesno, levels=c("yes", "no")); yesnofac
relevel(yesnofac, ref="yes")
as.numeric(yesnofac)
# using the mutate function
library(Hmisc); library(plyr)
acs.new <- mutate(acs, new_pwgtp80=cut2(pwgtp80,g=4)); table(acs.new$new_pwgtp80) # create a new version of a variable and simultaneously add it to a dataset
# Common transforms
abs(-2) # absolute value
sqrt(2)
ceiling(3.475) # 4
floor(3.475) # 3
round(3.475, digits=2) # 3.48
signif(3.475, digits=2) # 3.5
cos(2); sin(2)
log(2); log2(2); log10(2)
exp(2)

# Reshaping data
library(reshape2); head(mtcars)
# melting data frames
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars, id=c("carname", "gear", "cyl"), measure.vars = c("mpg", "hp")) # id vaiables and measure variables
head(carMelt, 3); tail(carMelt, 3) # in variable column you see both mpg and hp; in value column you see the values for mpg and hp variables
# casting data frames
cylData <- dcast(carMelt, cyl ~ variable); cylData # reformatting the dataset: see the cyl breaken down by different variables; summarizes the dataset by length: for cyl "4" we have 11 measures of mpg and 11 measures of hp
cylData <- dcast(carMelt, cyl ~ variable, mean); cylData # take mean for each value
# averaging values
head(InsectSprays)
tapply(InsectSprays$count, InsectSprays$spray, sum) # apply to count along the index spray the sum fun
spIns <- split(InsectSprays$count, InsectSprays$spray); spIns # split counts by different spray
sprCount <- lapply(spIns, sum); sprCount # for each list element we apply sum
unlist(sprCount) # combine, convert list to vector
sapply(spIns, sum) # the same
ddply(InsectSprays,.(spray), summarise, sum=sum(count)) # spray to summarise
spraySums <- ddply(InsectSprays,.(spray), summarise, sum=ave(count, FUN=sum)); dim(spraySums)
head(spraySums) # apply the sum function to each variable in the original dataset
acast() # to convert to array
arrange()

# Managing data frames with dplyr
library(dplyr)
options(width = 105)
head(mtcars); dim(mtcars); str(mtcars)
names(mtcars)
# select
head(select(mtcars, disp:vs)) # select all columns between disp and vs
head(select(mtcars, -(disp:vs))) # select all columns except those between disp and vs
i <- match("disp", names(mtcars)); j <- match("vs", names(mtcars)); head(mtcars[, -(i:j)]) # the same
# filter
mt <- filter(mtcars, mpg > 20); mt
mt <- filter(mtcars, mpg > 20 & disp < 100); mt
# arrange - reorder df rows based on the values of df columns
c <- arrange(mtcars, hp); c
c <- arrange(mtcars, desc(hp)); c
# rename
mt <- rename(mtcars, cylinder = cyl, horsepower = hp); mt
# mutate - to transform the existing variable or create the new ones
mt <- mutate(mtcars, mpgtrend = mpg-mean(mpg, na.rm=T)); head(select(mt, mpg, mpgtrend))
# groupby - split a dataframe according to certain categorical variables
mt <- mutate(mtcars, newcyl = factor(1 * (cyl > 4), labels = c("low", "high"))); mt
highcyl <- group_by(mt, newcyl); highcyl
# summarize
summarize(highcyl, mpgn = mean(mpg, na.rm = T), hpn = max(hp), wtn = median(wt))
# %>% operator to chain different operations together, so basically you don't need to specify the dataframe of the previous operation or output
mtcars %>% mutate(newcyl = factor(1 * (cyl > 4), labels = c("low", "high"))) %>% group_by(newcyl) %>% summarize(mpgn = mean(mpg, na.rm = T), hpn = max(hp), wtn = median(wt))

# Merging data
setwd('/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/getting_and_cleaning_data')
url1 <- "https://dl.dropboxusercontent.com/u/7710864/data/reviews-apr29.csv"
url2 <- "https://dl.dropboxusercontent.com/u/7710864/data/solutions-apr29.csv"
download.file(url1, destfile = "./w3/reviews.csv", method = "curl")
download.file(url2, destfile = "./w3/solutions.csv", method = "curl")
reviews <- read.csv("./w3/reviews.csv"); solutions <- read.csv("./w3/solutions.csv")
# merge() x,y - dataframes; by, by.x, by.y - by which column to merge; by default it will merge by all columns that have the same name in both dataframes (by); all = T include rows with missing values
mergeData <- merge(reviews, solutions, by.x="solution_id", by.y="id", all=TRUE) # merge dataframes by solution_id in reviews and by id in solutions dataframe
intersect(names(solutions), names(reviews))
mergeData2 <- merge(reviews, solutions, all=TRUE) # merge by all common column names
# join
df1 <- data.frame(id=sample(1:10), x=rnorm(10))
df2 <- data.frame(id=sample(1:10), y=rnorm(10))
arrange(join(df1, df2), id)
# join multiple dataframes
df1 <- data.frame(id=sample(1:10), x=rnorm(10))
df2 <- data.frame(id=sample(1:10), y=rnorm(10))
df3 <- data.frame(id=sample(1:10), z=rnorm(10))
dfList <- list(df1, df2, df3)
join_all(dfList)

# Week quiz 3
# Q1 - 125, 238, 262
url1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(url1, destfile = "./w3/Q1.csv", method = "curl")
df <- read.csv("./w3/Q1.csv")
df$ACR # == 3
df$AGS # == 6
#agricultureLogical <- filter(df, ACR == 3 & AGS == 6)
agricultureLogical <- (df$ACR == 3 & df$AGS == 6)
which(agricultureLogical) # 125, 238, 262
# Q2 - -15259150 -10575416 
library(httr); library(jpeg)
GET("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg", write_disk("./w3/Q2.jpg"))
j <- readJPEG("./w3/Q2.jpg", native=T)
quantile(j, na.rm = T, probs = c(0.3, 0.8)) # -15259150 -10575416 
# Q3 - 189 matches, 13th country is St. Kitts and Nevis
gdp_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
edu_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(gdp_url, destfile = "./w3/Q3_gdp.csv", method = "curl")
download.file(edu_url, destfile = "./w3/Q3_edu.csv", method = "curl")
gdp <- read.csv("./w3/Q3_gdp.csv")
edu <- read.csv("./w3/Q3_edu.csv")
head(gdp,3); head(edu,3)
# GDP cleaning
k <- gdp %>% select(c(X, Gross.domestic.product.2012, X.2)) %>% rename(CountryCode = X, Ranking = Gross.domestic.product.2012, Economy=X.2)
k1 <- k[which(k$CountryCode > 0 & k$Ranking > 0),]
k2 <- arrange(join(edu, k1), CountryCode)
k2 <- merge(edu, k1) # 189
k2 <- merge(k1, edu) # 189
k2$Ranking <- as.numeric(k2$Ranking)
k3 <- k2[order(k2$Ranking,decreasing=TRUE),]; k3[13, "Economy"]
# Q4 - 32.96667, 91.91304
names(c_des)
c_des$Income.Group
hi_OECD <- filter(c_des, Income.Group=="High income: OECD"); hi_OECD
hi_nonOECD <- filter(c_des, Income.Group=="High income: nonOECD"); hi_nonOECD
av_OECD <- mean(hi_OECD$Ranking); av_OECD
av_nonOECD <- mean(hi_nonOECD$Ranking); av_nonOECD
# Q5 - 5
library(Hmisc)
c_des$newgr <- cut2(c_des$Ranking, g=5); table(c_des$newgr, c_des$Income.Group) # 5

# Week 4
# Editing text variables
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/getting_and_cleaning_data")
edu_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(edu_url, destfile = "./w3/Q3_edu.csv", method = "curl")
edu <- read.csv("./w3/Q3_edu.csv"); head(edu,3)
names(edu); tolower(names(edu)); toupper(names(edu)) # make all the letters lowercase
# fixing character vectors - strsplit()
splitNames <- strsplit(names(edu), "\\."); splitNames[[2]] # split by period, use \\ escape character because period is reserved
# quick aside - lists
mylist <- list(letters = c("A", "b", "c"), numbers = 1:3, matrix(1:25, ncol=5)); head(mylist)
mylist[1]; mylist$letters; mylist[[1]]
splitNames[[2]][1] # split and take the first element
firstElement <- function(x) {x[1]}
sapply(splitNames, firstElement)
# fixing character vectors - sub(), gsub()
edu$new_var <- rnorm(234); names(edu); sub("_", "", names(edu),); sub("\\.", "", names(edu),)
test <- "this_is_test"; sub("_", "", test); gsub("_", "", test); gsub("\\.", "", names(edu),)
# finding values - grep(), grepl()
grep("System", gsub("\\.", "", names(edu),)) # return the index of the found variable
grep("Republic", edu$Long.Name)
table(grepl("System", gsub("\\.", "", names(edu),))) # return number of variables having the pattern
newt <- edu[!grepl("System", gsub("\\.", "", names(edu),)),] # return only those rows that don't have System in the value
grep("System", gsub("\\.", "", names(edu),), value=T)
grep("Lol", edu$Long.Name); length(grep("Lol", edu$Long.Name)) # a way to check if the value exist
# stringr package
install.packages("stringr"); library(stringr)
nchar("System"); substr("System", 1, 3) # to find the first through the third letters
paste("Sys", "tem"); paste0("Sys", "tem") # paste without space
strtrim("System   ", 5)

# Regular expressions
# Working with dates
d1 <- date(); d1; class(d1)
d2 <- Sys.Date(); d2; class(d2)
format(d2, "%a %b %d")
x <- c("1jan1960", "2jan1960", "31mar1960", "30jul1960"); z <- as.Date(x, "%d%b%Y"); z
z[2] - z[1]; as.numeric(z[2] - z[1])
# converting to julian
weekdays(d2); months(d2); julian(d2)
# lubridate package
install.packages("lubridate"); library(lubridate)
ymd("20140108"); mdy("08/04/2013"); dmy("03-04-2013") # convert number to a date
ymd_hms("2011-02-10 10:15:03")
ymd_hms("2011-02-10 10:15:03", tz="Pacific/Auckland")
?Sys.timezone
x <- dmy(c("1jan1960", "2jan1960", "31mar1960", "30jul1960"))
wday(x[1]); wday(x[1], label = T)

# Week 4 Quiz
# Q1 - "" "15"
url1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(url1, destfile = "./w4/Q1.csv", method = "curl")
acs <- read.csv("./w4/Q1.csv")
names(acs)
splitNames <- strsplit(names(acs), "wgtp"); splitNames[123] # "" "15"
# Q2 - 377652.4
gdp_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(gdp_url, destfile = "./w4/Q2_gdp.csv", method = "curl")
gdp <- read.csv("./w4/Q2_gdp.csv")
head(gdp,3)
# GDP cleaning
library(dplyr)
k <- gdp %>% select(c(X, Gross.domestic.product.2012, X.2, X.3)) %>% rename(CountryCode = X, Ranking = Gross.domestic.product.2012, Economy=X.2, Money=X.3)
k1 <- k[which(k$CountryCode > 0 & k$Ranking > 0),]
k1$Money
k1$Money_new <- as.numeric(gsub(",", "", k1$Money))
mean(k1$Money_new, na.rm=T) # 377652.4
# Q3 - grep("^United",countryNames), 3
# to count the number of countries whose name begins with "United", should be 3
grep("*United",k1$Economy) # nope
grep("^United",k1$Economy) # this one returns indeces for three found variables
grep("*United",k1$Economy) # nope
grep("United$",k1$Economy) # nope
# Q4 - 13
gdp_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
edu_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(gdp_url, destfile = "./w4/Q4_gdp.csv", method = "curl")
download.file(edu_url, destfile = "./w4/Q4_edu.csv", method = "curl")
gdp <- read.csv("./w4/Q4_gdp.csv")
edu <- read.csv("./w4/Q4_edu.csv")
head(gdp,3); head(edu,3)
# GDP cleaning
k <- gdp %>% select(c(X, Gross.domestic.product.2012, X.2)) %>% rename(CountryCode = X, Ranking = Gross.domestic.product.2012, Economy=X.2)
k1 <- k[which(k$CountryCode > 0 & k$Ranking > 0),]
k2 <- merge(edu, k1) # 189
k2 <- merge(k1, edu) # 189
#k2$Ranking <- as.numeric(k2$Ranking)
#k3 <- k2[order(k2$Ranking,decreasing=TRUE),]; k3[13, "Economy"]
names(k2)
fyear <- k2$Special.Notes
grep("*(Fiscal year end: June)",k2$Special.Notes) # 13
# Q5 - 47
install.packages("quantmod"); library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
length(grep("*(2012)", sampleTimes)) # 250
sampleTimes2012 <- sampleTimes[grep("*(2012)", sampleTimes)]
# Monday is 2
co <- 0
for(i in 1:length(sampleTimes2012)){
  if(wday(sampleTimes2012[i], label = T)=="Mon"){
    print("This is Monday ")
    co <- co + 1
  }
}
co # 47

# Week 4 PA
```

## Exploratory Data Analysis (Coursera)

```{r Exploratory_Data_Analysis, echo=FALSE}
## Week 1
# L1 Graphs
# Principles of Analytic Graphics
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/exploratory_data_analysis")
pollution <- read.csv("./file.csv", colClasses = c("numeric", "character", "factor", "numeric"))
head(pollution)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w1/acs.csv", method = "curl")
acs <- read.csv("./w1/acs.csv"); head(acs,3)
# five-number summary
summary(acs$pwgtp80)
# boxplot
boxplot(acs$pwgtp80, col = "blue")
abline(h = 200, lwd = 2) # overlying features: 200 could be a standard number, smth you sompared with
# histogram
hist(acs$pwgtp80, col = "green", breaks = 100) # play around with the breaks
rug(acs$pwgtp80) # add a Rug (all the points) to a plot
abline(v = median(acs$pwgtp80), col = "magenta", lwd = 4) # v - for vertical line
# barplot
barplot(table(acs$PUMA), col = "wheat", main = "Number of observations in each PUMA")
# multiple boxplots
boxplot(acs$pwgtp80 ~ acs$PUMA, data = acs, col = "red")
# multiple histograms
par(mfrow=c(2,1), mar = c(4,4,2,1))
hist(subset(acs, PUMA == 100)$pwgtp80, col = "green")
hist(subset(acs, PUMA == 200)$pwgtp80, col = "green")
# scatterplot
with(acs, plot(CIT, pwgtp80, col = PUMA)) # add color
abline(h = 200, lwd = 2, lty = 2)
# multiple scatterplots
par(mfrow=c(2,1), mar = c(4,4,2,1))
with(subset(acs, PUMA == 100), plot(CIT, pwgtp80, main = "PUMA 100"))
with(subset(acs, PUMA == 200), plot(CIT, pwgtp80, main = "PUMA 200"))

# L2 Plotting
# The base plotting system (the oldest one)
library(datasets)
data(cars)
with(cars, plot(speed, dist))
# The Lattice system
library(lattice)
state <- data.frame(state.x77, region = state.region)
xyplot(Life.Exp ~ Income | region, data = state, layout = c(4,1))
# The ggplot system
library(ggplot2)
data(mpg)
qplot(displ, hwy, data = mpg)
# The base plotting system
library(datasets)
hist(airquality$Ozone)
with(airquality, plot(Wind, Ozone)); title(main = "Ozone and Wind in New York City")
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City"))
with(subset(airquality, Month == 5), points(Wind, Ozone, col = "blue"))
airquality <- transform(airquality, Month = factor(Month))
boxplot(Ozone ~ Month, airquality, xlab = "Month", ylab = "Ozone (ppb)")
par("lty"); par("col")
par("mar") # 5.1 - bottom (first), 4.1 - left-hand side (second), 4.1 - top (third), 2.1 - right-hand side (fourth). Clock-wise direction. 5.1 means 5.1 lines of text
# Base plot with annotation
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City", type = "n")) # set everything but to not plot anything
with(subset(airquality, Month == 5), points(Wind, Ozone, col = "blue"))
with(subset(airquality, Month != 5), points(Wind, Ozone, col = "red"))
legend("topright", pch = 1, col = c("blue", "red"), legend = c("May", "Other months"))
# Base plot with regression line
with(airquality, plot(Wind, Ozone, main = "Ozone and Wind in New York City", pch = 20))
model <- lm(Ozone ~ Wind, airquality)
abline(model, lwd = 2)
# Multiple base plots
par(mfrow=c(1,2)) # one row, two columns
with(airquality, {
  plot(Wind, Ozone, main = "Ozone and Wind")
  plot(Solar.R, Ozone, main = "Ozone and Solar Radiation")
})
# Multiple base plots
par(mfrow=c(1,3), mar=c(4,4,2,1), oma=c(0,0,2,0))
with(airquality, {
  plot(Wind, Ozone, main = "Ozone and Wind")
  plot(Solar.R, Ozone, main = "Ozone and Solar Radiation")
  plot(Temp, Ozone, main = "Ozone and Temperature")
  mtext("Ozone and Weather in New York City", outer = TRUE)
})
# Demo
x <- rnorm(100)
hist(x)
y <- rnorm(100)
plot(x, y)
par(mar=c(2,2,2,2)); plot(x, y)
par(mar=c(4,4,2,2)); plot(x, y)
example(points) # check the pch type
x <- rnorm(100); y <- rnorm(100)
plot(x, y, pch=20)
title("Scatterplot")
text(-2,-2, "label")
legend("topleft", legend = "Data", pch=20)
z <- rpois(100,2)
# Multiple base plots
par(mfrow=c(2,1))
plot(x, y, pch=20)
plot(x, z, pch=19)
# Example
par(mfrow=c(1,1))
x <- rnorm(100); y <- x + rnorm(100); g <- gl(2, 50, labels = c("Male", "Female"))
plot(x, y, type = "n")
points(x[g == "Male"], y[g == "Male"], col = "blue")
points(x[g == "Female"], y[g == "Female"], col = "red")

# L3 Graphics Devices - smth where you can make a plot appear: e.g. screen or file devices
?Devices
# vector formats
?pdf(); ?svg(); ?win.metafile; ?postscript
# bitmap formats
?png(); ?jpeg(); ?tiff(); ?bmp()
dev.cur() # the currently active graphics device can be found by calling
dev.set() # to change the active graphics device
# copying plots
dev.copy() # to copy a plot from one device to another
dev.copy2pdf() # copy to a pdf file
# exmaple of copying from screen device to a png file device, a faster way!
x <- rnorm(100); y <- rnorm(100); plot(x, y, pch=20)
dev.copy(png, file = "test.png") # copy a plot to a png file, but the result may not be the identical to the original!
dev.off() # to close the png device!

# Week 1 Quiz
# Q1 - Show comparisons
# Q2 - They are typically made very quickly.
# Q3 - Plots are created and annotated with separate functions
# Q4 - A PNG file
# Q5 - SVG
# Q6 - Scatterplots with many many points
# Q7 - lines()
# Q8 - windows()
# Q9 - the plotting symbol/character in the base graphics system
# Q10 - Construct the plot on the screen device and then copy it to a PDF file with dev.copy2pdf()

# Week 1 PA

## Week 2
# L1. Lattice Plotting System - all plotting/annotation is done at once with a single function call
?xyplot() # scatterplots
?bwplot() # box-and-whickers plot, boxplots
?histogram() # histogram
?stripplot() # a boxplot with actual points
?dotplot() # plot dots on "violin strings"
?splom() # scatterplot matrix, similar to pairs in base plotting system
?levelplot(); ?contourplot() # fo rplotting "image" data
# simple lattice plot
library(lattice); library(datasets)
xyplot(Ozone ~ Wind, data = airquality)
# multidimentional panel
airquality <- transform(airquality, Month = factor(Month))
xyplot(Ozone ~ Wind | Month, data = airquality, layout = c(5,1)) # auto-printing
# lattice behaviour
p <- xyplot(Ozone ~ Wind, data = airquality)
print(p)
# panel functions
set.seed(10)
x <- rnorm(100); f <- rep(0:1, each=50); y <- x + f - f * x + rnorm(100, sd=0.5)
f <- factor(f, labels=c("group 1", "group 2"))
xyplot(y ~ x | f, layout = c(2,1)) # plot with two panels
# custom panel function
xyplot(y ~ x | f, panel = function(x, y, ...) {
  panel.xyplot(x, y, ...) # first call the default panel function for xyplot
  panel.abline(h = median(y), lty = 2) # add the horizontal line at the median
})
xyplot(y ~ x | f, panel = function(x, y, ...) {
  panel.xyplot(x, y, ...) # first call the default panel function for xyplot
  panel.lmline(x, y, col = 2) # overlay a simple linear regression line
})

# L2. ggplot2
library(ggplot2); str(mpg)
qplot(displ, hwy, data=mpg, color=drv)
qplot(displ, hwy, data=mpg, geom=c("point", "smooth"))
qplot(displ, data=mpg, geom="density")
qplot(hwy, data=mpg, fill=drv) # histogram
qplot(displ, hwy, data=mpg, facets=.~drv) # add facets (like panels) column-wise (~right)
qplot(hwy, data=mpg, facets=drv~., binwidth=2) # add facets (like panels) row-wise (left~)
qplot(displ, hwy, data=mpg, color=drv) + geom_smooth(method = "lm")
?coord_cartesian(ylim=c(-3,3)) # to include but not show outliers
?cut() # to categorize the continuous variable to a series of ranges so we can condition on them

# Week 2 Quiz
# Q1 - an object of class "trellis"
# Q2 - A set of 3 panels showing the relationship between weight and time for each diet.
library(nlme)
library(lattice)
xyplot(weight ~ Time | Diet, BodyWeight)
# Q3 - lpoints()
# Q4 - The object 'p' has not yet been printed with the appropriate print method.
library(lattice)
library(datasets)
data(airquality)
p <- xyplot(Ozone ~ Wind | factor(Month), data = airquality)
# Q5 - trellis.par.set()
# Q6 - the Grammar of Graphics developed by Leland Wilkinson
# Q7 - 
library(datasets); data(airquality)
qplot(Wind, Ozone, data = airquality, facets = . ~ factor(Month))
# Q8 - a plotting object like point, line, or other shape
# Q9 - ggplot does not yet know what type of layer to add to the plot.
library(ggplot2); library(ggplot2movies)
g <- ggplot(movies, aes(votes, rating))
print(g)
# Q10 - qplot(votes, rating, data = movies) + geom_smooth()
qplot(votes, rating, data = movies)

## Week 3
# L1. Hierarchical clustering
set.seed(1234)
par(mar = c(0,0,0,0))
x <- rnorm(12, mean = rep(1:3, each=4), sd=0.2)
y <- rnorm(12, mean = rep(c(1,2,1), each=4), sd=0.2)
plot(x, y, col="blue", pch=19, cex=2)
text(x + 0.05, y + 0.05, labels=as.character(1:12))
df <- data.frame(x=x, y=y)
# calculate the paitwise distances between the points
distxy <- dist(df)
hcldistxy <- hclust(distxy); plot(hcldistxy) # the next step is to cut the tree to get the number of clusters
# prettier dendrograms
myplclust <- function(hclust, lab=hclust$labels, lab.col=rep(1,length(hclust$labels)), hang=0.1,...){
  y <- rep(hclust$height, 2)
  x <- as.numeric(hclust$merge)
  y <- y[which(x<0)]
  x <- y[which(x<0)]
  x <- abs(x)
  y <- y[order(x)]
  x <- x[order(x)]
  plot(hclust, labels=FALSE, hang=hang, ...)
  text(x=x, y=y[hclust$order]-(max(hclust$height)*hang), labels=lab[hclust$order],
       col=lab.col[hclust$order], srt=90, adj=c(1, 0.5), xpd=NA, ...)
}
myplclust(hcldistxy, lab=rep(1:3, each=4), lab.col=rep(1:3,each=4))
# how do you merge points together? by default "complete" method
# heatmap
df <- data.frame(x=x, y=y)
set.seed(143)
dm <- as.matrix(df)[sample(1:12),]
heatmap(dm) # runs hclust on rows and columns of the table

## L2. K-means clustering and dimension reduction
# K-means clustering
set.seed(1234)
par(mar = c(0,0,0,0))
x <- rnorm(12, mean = rep(1:3, each=4), sd=0.2)
y <- rnorm(12, mean = rep(c(1,2,1), each=4), sd=0.2)
plot(x, y, col="blue", pch=19, cex=2)
text(x + 0.05, y + 0.05, labels=as.character(1:12))
df <- data.frame(x=x, y=y)
km <- kmeans(df, centers=3); names(km); km$cluster
par(mar=rep(0.2,4))
plot(x, y, col=km$cluster, pch=19, cex=2)
points(km$centers, col=1:3, pch=3, cex=3, lwd=3)
# heatmap
set.seed(1234)
dm <- as.matrix(df)[sample(1:12),]
km2 <- kmeans(dm, centers=3)
par(mfrow=c(1,2), mar=c(2,4,0.1,0.1))
image(t(dm)[, nrow(dm):1], yaxt="n"); image(t(dm)[, order(km2$cluster)], yaxt="n")

# Dimension reduction: PCA and singular value decomposition
set.seed(12345)
par(mar=rep(0.2,4))
dm <- matrix(rnorm(400), nrow=40)
image(1:10, 1:40, t(dm)[, nrow(dm):1])
heatmap(dm)
set.seed(678910) # what id we add a pattern?
for(i in 1:40){
  # flip a coin
  cF <- rbinom(1, size=1, prob=0.5)
  if (cF) {
    dm[i,] <- dm[i,] + rep(c(0,3), each=5)
  }
}
image(1:10, 1:40, t(dm)[, nrow(dm):1]); heatmap(dm)
# patterns in rows and columns
hh <- hclust(dist(dm))
dmordered <- dm[hh$order,]
par(mfrow=c(1,3))
image(t(dmordered)[, nrow(dmordered):1])
plot(rowMeans(dmordered), 40:1, xlab = "Row Mean", ylab = "Row", pch = 19)
plot(colMeans(dmordered), xlab = "Column", ylab = "Column Mean", pch = 19)
# SVD
svd1 <- svd(scale(dmordered)) # list of three matrices u, d and v
par(mfrow=c(1,3))
image(t(dmordered)[, nrow(dmordered):1])
plot(svd1$u[, 1], 40:1, xlab = "Row", ylab = "First left singular vector", pch = 19)
plot(svd1$v[, 1], xlab = "Column", ylab = "First right singular vector", pch = 19)
# variance explained
par(mfrow=c(1,2))
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19)
# relationship to principal components is close to svd
svd1 <- svd(scale(dmordered))
pca1 <- prcomp(dmordered, scale=T)
plot(pca1$rotation[,1], svd1$v[,1], pch=19, xlab="Principal component 1", ylab="Right Singular Vector 1"); abline(c(0,1))
# variance explained
constantMatrix <- dmordered*0
for(i in 1:dim(dmordered)[1]){
  constantMatrix[i,] <- rep(c(0,1), each=5)
}
svd1 <- svd(constantMatrix)
par(mfrow=c(1,3))
image(t(constantMatrix)[, nrow(constantMatrix):1])
plot(svd1$d, xlab = "Column", ylab = "Singular value", pch = 19)
plot(svd1$d^2/sum(svd1$d^2), xlab = "Column", ylab = "Prop. of variance explained", pch = 19) # variance can be explained by a single component
# add a second pattern to a dataset
set.seed(678910)
for(i in 1:40){
  # flip a coin
  cf1 <- rbinom(1, size=1, prob=0.5)
  cf2 <- rbinom(1, size=1, prob=0.5)
  if (cf1) {
    dm[i,] <- dm[i,] + rep(c(0,5), each=5)
  }
  if (cf2) {
    dm[i,] <- dm[i,] + rep(c(0,5), 5)
  }
}
hh <- hclust(dist(dm))
dmordered <- dm[hh$order,]
svd2 <- svd(scale(dmordered))
par(mfrow=c(1,3))
image(t(dmordered)[, nrow(dmordered):1])
# true patterns
plot(rep(c(0,1), each=5), xlab = "Column", ylab = "Pattern 1", pch = 19)
plot(rep(c(0,1), 5), xlab = "Column", ylab = "Pattern 2", pch = 19)
# v and patterns in svd2
svd2 <- svd(scale(dmordered))
par(mfrow=c(1,3))
image(t(dmordered)[, nrow(dmordered):1])
plot(svd2$v[,1], xlab = "Column", ylab = "First right singular vector", pch = 19)
plot(svd2$v[,2], xlab = "Column", ylab = "Second right singular vector", pch = 19)
# d and variance explained
svd2 <- svd(scale(dmordered))
par(mfrow=c(1,2))
plot(svd2$d, xlab = "Column", ylab = "Singular", pch = 19)
plot(svd2$d^2/sum(svd2$d^2), xlab = "Column", ylab = "Percent of variance explained", pch = 19)

# Dealing with missing values
# Imputing
library(BiocManager); BiocManager::install("impute"); library(impute)
dm2 <- dmordered
dm2[sample(1:100, size=40, replace=FALSE)] <- NA
dm2 <- impute.knn(dm2)$data
svd1 <- svd(scale(dmordered)); svd2 <- svd(scale(dm2))
par(mfrow=c(1,2))
plot(svd1$v[,1],pch=19); plot(svd2$v[,1],pch=19) # very similar

# face exmaple
load("data/face.rda"); image(t(faceData)[, nrow(faceData):1])
# variance explained
svd1 <- svd(scale(faceData)); plot(svd1$d^2/sum(svd1$d^2), xlab = "Singular vector", ylab = "Percent of variance explained", pch = 19)
# create approximations that use fewer variables
svd1 <- svd(scale(faceData))
approx1 <- svd1$u[,1] %*% t(svd1$v[,1]) %*% svd1$d[1] # use the first singular vector
approx5 <- svd1$u[,1:5] %*% t(svd1$v[,1:5]) %*% diag(svd1$d[1:5]) # use first five singular vectors; make diagonal matrix of d
approx10 <- svd1$u[,1:10] %*% t(svd1$v[,1:10]) %*% diag(svd1$d[1:10]) # use first 10 singular vectors; make diagonal matrix of d
# plot approximations
par(mfrow=c(1,4))
image(t(approx1)[, nrow(approx1):1], main="(a)")
image(t(approx5)[, nrow(approx5):1], main="(b)")
image(t(approx10)[, nrow(approx10):1], main="(c)")
image(t(faceData)[, nrow(faceData):1], main="(d)") # original data

# L3 Working with color
?grDevices; library(help = "grDevices")
?colorRamp(); ?colorRampPalette
?colors()
pal <- colorRamp(c("red", "blue")) # returns a function called pal
pal(0) # RGB in columns: red (1st column), green (second column), blue (third column); get red color
pal(1) # blue
pal(0.5) # pink
pal(seq(0,1,len=10)) # get sequence of colors between red and blue

pal <- colorRampPalette(c("red", "yellow")) # returns a function called pal
pal(2) # returns two colors in HEX format
pal(10) # returns ten colors in HEX format

library(help = "RColorBrewer"); library(RColorBrewer)
cols <- brewer.pal(3, "BuGn"); cols # returns three colors from the BuGn palette
pal <- colorRampPalette(cols)
image(volcano, col=pal(20))

x <- rnorm(10000); y <- rnorm(10000)
smoothScatter(x,y) # useful when you need to plot lots of points! It create a 2D histogram; high density areas (with lots of points) - darker

library(help = "colorspace") # for different control over colors

?make.names()

## Week 4 - PA
# Downloading
#setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/exploratory_data_analysis/w4_pa")
fileURL <- "https://d396qusza40orc.cloudfront.net/exdata%2Fdata%2FNEI_data.zip"
temp <- tempfile(); download.file(fileURL,temp)
file.copy(from = temp, to = paste0(getwd(),"/raw_data.zip")) # download zip file as raw_data.zip
unlink(temp) # delete the temp file or use file.remove(temp)
unzip("raw_data.zip") # unzip the raw_data.zip
# Reading PM2.5 Emissions Data (NEI) and Source Classification Code Table (SCC)
NEI <- readRDS("summarySCC_PM25.rds")
SCC <- readRDS("Source_Classification_Code.rds")

# The overall goal of this assignment is to explore the National Emissions Inventory database and see what it say about fine particulate matter pollution in the United states over the 10-year period 1999–2008.

# Plot 1
# Q1: Have total emissions from PM2.5 decreased in the United States from 1999 to 2008? Using the base plotting system, make a plot showing the total PM2.5 emission from all sources for each of the years 1999, 2002, 2005, and 2008.
#head(NEI)
library(dplyr)
options(width = 105, scipen = 7)
q1 <- tapply(NEI$Emissions, NEI$year, sum) # get total emissions from 199 to 2008
df1 <- as.data.frame(q1); colnames(df1) <- "total_emissions"
#df1$year <- rownames(df1); df1 <- select(df1, year, total_emissions); df1
#df1$year <- factor(rownames(df1)); df1 <- select(df1, year, total_emissions); df1
#df1$year <- as.Date(df1$year, "%Y")
df1 <- df1[1:4, "total_emissions"]

png(filename = "plot1.png", width = 7, height = 6, units = "in", res = 300)
par(mfrow=c(1,1))
barplot(df1, names.arg = c("1999", "2002", "2005", "2008"),
        main = "Total emissions from PM2.5 in the USA from 1999 to 2008",
        xlab = "Year", ylab = "Total PM2.5 emissions, in tons")
dev.off()

# Plot 2
# Q2: Have total emissions from PM2.5 decreased in the Baltimore City, Maryland (fips == "24510") from 1999 to 2008? Use the base plotting system to make a plot answering this question.
options(width = 105, scipen = 7)
balt.df <- NEI[NEI$fips=="24510",] # get emissions for Baltimore City, Maryland (fips == "24510")
q2 <- tapply(balt.df$Emissions, balt.df$year, sum) # get total emissions from 1999 to 2008
df2 <- as.data.frame(q2); colnames(df2) <- "total_emissions"
df2 <- df2[1:4, "total_emissions"]
#df2$year <- rownames(df2); df2 <- select(df2, year, total_emissions)

png(filename = "plot2.png", width = 7, height = 6, units = "in", res = 300)
par(mfrow=c(1,1))
barplot(df2, names.arg = c("1999", "2002", "2005", "2008"),
        main = "Total emissions from PM2.5 in the Baltimore City from 1999 to 2008",
        xlab = "Year", ylab = "Total PM2.5 emissions, in tons")
dev.off()

# Plot 3
# Q3: Of the four types of sources indicated by the type (point, nonpoint, onroad, nonroad) variable, which of these four sources have seen decreases in emissions from 1999–2008 for Baltimore City? Which have seen increases in emissions from 1999–2008? Use the ggplot2 plotting system to make a plot answer this question.
library(plyr); library(ggplot2); library(reshape2)
options(width = 105, scipen = 7)
#unique(NEI$type)
point.df <- NEI[NEI$type=="POINT",]; point.df <- tapply(point.df$Emissions, point.df$year, sum)
nonpoint.df <- NEI[NEI$type=="NONPOINT",]; nonpoint.df <- tapply(nonpoint.df$Emissions, nonpoint.df$year, sum)
onroad.df <- NEI[NEI$type=="ON-ROAD",]; onroad.df <- tapply(onroad.df$Emissions, onroad.df$year, sum)
nonroad.df <- NEI[NEI$type=="NON-ROAD",]; nonroad.df <- tapply(nonroad.df$Emissions, nonroad.df$year, sum)

point.df <- as.data.frame(point.df); colnames(point.df) <- "point"
nonpoint.df <- as.data.frame(nonpoint.df); colnames(nonpoint.df) <- "nonpoint"
onroad.df <- as.data.frame(onroad.df); colnames(onroad.df) <- "onroad"
nonroad.df <- as.data.frame(nonroad.df); colnames(nonroad.df) <- "nonroad"
point.df$year <- rownames(point.df); point.df <- select(point.df, year, point); point.df
nonpoint.df$year <- rownames(nonpoint.df); nonpoint.df <- select(nonpoint.df, year, nonpoint); nonpoint.df
onroad.df$year <- rownames(onroad.df); onroad.df <- select(onroad.df, year, onroad); onroad.df
nonroad.df$year <- rownames(nonroad.df); nonroad.df <- select(nonroad.df, year, nonroad); nonroad.df

df_list <- list(point.df, nonpoint.df, onroad.df, nonroad.df)
df3 <- join_all(df_list)
test <- melt(df3, id=c("year"), measure.vars = c("point", "nonpoint", "onroad", "nonroad"))

png(filename = "plot3.png", width = 10, height = 6, units = "in", res = 300)
ggplot(data=test, aes(x=year, y=value, fill=variable)) + 
  geom_bar(position="dodge", stat="identity") +
  theme_classic() +
  scale_fill_brewer(name = "Four sources:", palette="Dark2") +
  labs(title = "Total emissions from PM2.5 from four sources from 1999–2008 for Baltimore City") +
  ylab("Total PM2.5 emissions, in tons") + xlab("Year")
dev.off()


# Plot 4
# Q4: Across the United States, how have emissions from coal combustion-related sources changed from 1999–2008?
library(plyr); library(ggplot2); library(reshape2)
options(width = 105, scipen = 7)
emm_sources <- unique(SCC$EI.Sector) # get all the sources
coal_sources <- emm_sources[grep("[Cc]oal", emm_sources)] # get coal combustion-related sources
scc_coal <- SCC[SCC$EI.Sector %in% coal_sources,]
scc_coal_codes <- unique(scc_coal$SCC) # get coal SCC codes
q4 <- NEI[NEI$SCC %in% scc_coal_codes,]
df4 <- tapply(q4$Emissions, q4$year, sum)
df4 <- as.data.frame(df4); colnames(df4) <- "total_emissions"
df4$year <- rownames(df4); df4 <- select(df4, year, total_emissions); df4

png(filename = "plot4.png", width = 8, height = 6, units = "in", res = 300)
ggplot(df4, aes(x=year, y=total_emissions)) + 
  geom_bar(stat="identity") +
  theme_classic() +
  labs(title = "Total emissions from PM2.5 from coal combustion-related sources from 1999–2008") +
  ylab("Total PM2.5 emissions, in tons") + xlab("Year")
dev.off()

# Plot 5
# Q5: How have emissions from motor vehicle sources changed from 1999–2008 in Baltimore City?
library(plyr); library(ggplot2); library(reshape2)
options(width = 105, scipen = 7)
emm_sources <- unique(SCC$EI.Sector) # get all the sources
veh_sources <- emm_sources[grep("[Vv]ehicles", emm_sources)] # get motor vehicle sources
scc_veh <- SCC[SCC$EI.Sector %in% veh_sources,]
scc_veh_codes <- unique(scc_veh$SCC) # get motor vehicle SCC codes
q5 <- NEI[(NEI$SCC %in% scc_veh_codes) & (NEI$fips == "24510"),] # get motor vehicle emissions for Baltimore City, Maryland (fips == "24510")
df5 <- tapply(q5$Emissions, q5$year, sum)
df5 <- as.data.frame(df5); colnames(df5) <- "total_emissions"
df5$year <- rownames(df5); df5 <- select(df5, year, total_emissions); df5

png(filename = "plot5.png", width = 8, height = 6, units = "in", res = 300)
ggplot(df5, aes(x=year, y=total_emissions)) + 
  geom_bar(stat="identity") +
  theme_classic() +
  labs(title = "Total motor vehicle emissions from PM2.5 for Baltimore City from 1999–2008") +
  ylab("Total PM2.5 emissions, in tons") + xlab("Year")
dev.off()

# Plot 6
# Q6: Compare emissions from motor vehicle sources in Baltimore City with emissions from motor vehicle sources in Los Angeles County, California (fips == "06037". Which city has seen greater changes over time in motor vehicle emissions?
library(plyr); library(ggplot2); library(reshape2)
options(width = 105, scipen = 7)
emm_sources <- unique(SCC$EI.Sector) # get all the sources
veh_sources <- emm_sources[grep("[Vv]ehicles", emm_sources)] # get motor vehicle sources
scc_veh <- SCC[SCC$EI.Sector %in% veh_sources,]
scc_veh_codes <- unique(scc_veh$SCC) # get motor vehicle SCC codes
#q6 <- NEI[(NEI$SCC %in% scc_veh_codes) & (NEI$fips == "24510" | NEI$fips == "06037"),] # get motor vehicle emissions for Baltimore City and Los Angeles
q6_balt <- NEI[(NEI$SCC %in% scc_veh_codes) & (NEI$fips == "24510"),] # get motor vehicle emissions for Baltimore City
q6_la <- NEI[(NEI$SCC %in% scc_veh_codes) & (NEI$fips == "06037"),] # get motor vehicle emissions for Los Angeles
df6_balt <- tapply(q6_balt$Emissions, q6_balt$year, sum) # get total emissions
df6_la <- tapply(q6_la$Emissions, q6_la$year, sum) # get total emissions

df6_balt <- as.data.frame(df6_balt); colnames(df6_balt) <- "total_emissions"
df6_la <- as.data.frame(df6_la); colnames(df6_la) <- "total_emissions"

df6_balt$year <- rownames(df6_balt); df6_balt <- select(df6_balt, year, total_emissions); df6_balt
df6_la$year <- rownames(df6_la); df6_la <- select(df6_la, year, total_emissions); df6_la

merged <- merge(df6_balt, df6_la, by = "year")
colnames(merged) <- c("year", "Baltimore", "Los Angeles")
melted <- melt(merged, id=c("year"), measure.vars = c("Baltimore", "Los Angeles"))

png(filename = "plot6.png", width = 9, height = 6, units = "in", res = 300)
ggplot(melted, aes(x=year, y=value)) + 
  geom_bar(stat="identity") +
  theme_classic() +
  #scale_color_brewer(name = "Four sources:", palette="Dark2") +
  labs(title = "Total motor vehicle emissions from PM2.5 for Baltimore and Los Angeles from 1999–2008") +
  ylab("Total PM2.5 emissions, in tons") + xlab("Year") +
  facet_wrap(~ variable)
dev.off()
```

## Reproducible Research (Coursera)

```{r Reproducible_Research, echo=FALSE}
## Week 1 - Concepts, ideas & structure
library(kernlab); data(spam)
# perform the subsampling
set.seed(3435)
trainIndicator <- rbinom(4601, size=1, prob=0.5)
table(trainIndicator)
trainSpam <- spam[trainIndicator==1,]
testSpam <- spam[trainIndicator==0,]
# EDA
names(trainSpam); head(trainSpam)
table(trainSpam$type)
boxplot(capitalAve ~ type, data = trainSpam)
boxplot(log10(capitalAve + 1) ~ type, data = trainSpam)
pairs(log10(trainSpam[, 1:4] + 1)) # pairwise relationships between the different variables
hCluster = hclust(dist(t(trainSpam[, 1:57]))); plot(hCluster)
hClusterUpdated = hclust(dist(t(log10(trainSpam[, 1:55] + 1)))); plot(hClusterUpdated)
# Statistical modeling with single predictor
trainSpam$numType = as.numeric(trainSpam$type) - 1
costFunction = function(x, y) sum(x != (y > 0.5))
cvError = rep(NA, 55)
library(boot)
for (i in 1:55) {
  lmFormula = reformulate(names(trainSpam)[i], response = "numType")
  glmFit = glm(lmFormula, family = "binomial", data = trainSpam)
  cvError[i] = cv.glm(trainSpam, glmFit, costFunction,2)$delta[2]
}
## Which predictor has minimum cross-validated error?
names(trainSpam)[which.min(cvError)]
# get a measure of uncertainty
# use the best model from the group
predictionModel = glm(numType ~ charDollar, family = "binomial", data = trainSpam)
# get predictions on the test set
predictionTest = predict(predictionModel, testSpam)
predictedSpam = rep("nonspam", dim(testSpam)[1])
# classify as `spam' for those with prob > 0.5
predictedSpam[predictionModel$fitted > 0.5] = "spam"
table(predictedSpam, testSpam$type)
# error rate
(61 + 458)/(1346 + 458 + 61 + 449) # 0.2242869

## Week 2 - Markdown and knitr
# Week 2 PA
output_dir <- file.path(getwd(), "figure")
if (!dir.exists(output_dir)) {dir.create(output_dir)} # create figure directory
```

## Statistical Inference (Coursera)

```{r Statistical_Inference}
# Week 1
# Q1 - 11%
# Q2 - 0.75
# Q3 - p/(1-p) = Y/X
# Q4 - The median must be 0
# Q5 - 3
# Q6 - 40%

# Week 2
# Q1 - σ^2/n
# Q2 - How many sds below the mean 70 is? (70-80)/10 = -1;pnorm(-1) = ~16%
# Q3 - qnorm(.95, mean = 1100, sd = 75) = ~1223
# Q4 - qnorm(0.95, mean = 1100, sd = 75/sqrt(100)) ~1112
# Q5 - pbinom(3, size = 5, prob = 0.5, lower.tail = FALSE) 0.1875
# Q6 - pnorm(16, mean = 15, sd = 1) - pnorm(14, mean = 15, sd = 1) 0.6827
# Q7 - According to the LLN it should be near .5
# Q8 - ppois(10, lambda = 5*3) 0.1185

# Week 3


```




## Developing Data Products (Coursera)

```{r Developing_Data_Products}
## Week 1
#install.packages("shiny")
library(shiny)
# http://shiny.rstudio.com/tutorial/
setwd("/Users/foffa/Desktop/Learning/online_edu/coursera/data_science_specialisation_JHU/developing_data_products/my_app")
runApp() # to run app
runApp("path_to_the_app")
?builder # HTML tags in shiny

# ui.R
shinyUI(pageWithSidebar(
  headerPanel("Illustrating markup"),
  sidebarPanel(
      h1('Sidebar panel'),
      h1('H1 text'),
      h2('H2 Text'),
      h3('H3 Text'),
      h4('H4 Text')
  ),
  mainPanel(
      h3('Main Panel text'),
      code('some code'),
      p('some ordinary text')
  )
))

# sliderInput() # create slider and display it; labels have to match!
# ui.R
shinyUI(fluidPage(
  titlePanel("Slider App"),
  sidebarLayout(
    sidebarPanel(
      h1('Move the slider!'),
      sliderInput("slider2", "Slide Me!", 0, 100, 0)
  ),
  mainPanel(
      h3('Slider Value:'),
      textOutput("text1")
  ))
))
# server.R
shinyServer(function(input, output) {
  output$text1 = renderText(input$slider2)
})

# Illustrating inputs ui.R
# ui.R
shinyUI(pageWithSidebar(
    headerPanel("Illustrating inputs"),
    sidebarPanel(
        numericInput('id1', 'Numeric input, labeled id1', 0, min = 0, max = 10, step = 1),
        checkboxGroupInput("id2", "Checkbox",
                           c("Value 1" = "1",
                             "Value 2" = "2",
                             "Value 3" = "3")),
        dateInput("date", "Date:")  
    ),
    mainPanel(
        h3('Illustrating outputs'),
        h4('You entered'),
        verbatimTextOutput("oid1"),
        h4('You entered'),
        verbatimTextOutput("oid2"),
        h4('You entered'),verbatimTextOutput("odate")
    )
))
# server.R
shinyServer(function(input, output) {
    output$oid1 <- renderPrint({input$id1})
    output$oid2 <- renderPrint({input$id2})
    output$odate <- renderPrint({input$date})
})

# Building a prediction function
# ui.R
shinyUI(
  pageWithSidebar(
    # Application title
    headerPanel("Diabetes prediction"),
  
    sidebarPanel(
      numericInput('glucose', 'Glucose mg/dl', 90, min = 50, max = 200, step = 5),
      submitButton('Submit')
    ),
    mainPanel(
        h3('Results of prediction'),
        h4('You entered'),
        verbatimTextOutput("inputValue"),
        h4('Which resulted in a prediction of '),
        verbatimTextOutput("prediction")
    )
  )
)
# server.R
diabetesRisk <- function(glucose) glucose / 200

shinyServer(
  function(input, output) {
    output$inputValue <- renderPrint({input$glucose})
    output$prediction <- renderPrint({diabetesRisk(input$glucose)})
  }
)

# image example
# ui.R
shinyUI(pageWithSidebar(
  headerPanel("Example plot"),
  sidebarPanel(
    sliderInput('mu', 'Guess at the mean',value = 70, min = 62, max = 74, step = 0.05,)
  ),
  mainPanel(
    plotOutput('newHist')
  )
))
# server.R
library(UsingR)
data(galton)

shinyServer(
  function(input, output) {
    output$newHist <- renderPlot({
      hist(galton$child, xlab='child height', col='lightblue',main='Histogram')
      mu <- input$mu
      lines(c(mu, mu), c(0, 200),col="red",lwd=5)
      mse <- mean((galton$child - mu)^2)
      text(63, 150, paste("mu = ", mu))
      text(63, 140, paste("MSE = ", round(mse, 2)))
      })
    
  }
)

# Random numbers plot
# ui.R
library(shiny)
shinyUI(fluidPage(
    titlePanel("Plot Random Numbers"),
    sidebarLayout(
        sidebarPanel(
            numericInput("numeric", "How many random numbers should be plotted?",
                         value = 1000, min = 1, max = 1000, step = 1),
            sliderInput("sliderX", "Pick minimum and maximum x values",
                        -100, 100, value = c(-50, 50)),
            sliderInput("sliderY", "Pick minimum and maximum y values",
                        -100, 100, value = c(-50, 50)),
            checkboxInput("show_xlab", "Show/hide x axis label", value = TRUE),
            checkboxInput("show_ylab", "Show/hide y axis label", value = TRUE),
            checkboxInput("show_title", "Show/hide title")
        ),
        mainPanel(
            h3('graph of random points'),
            plotOutput("plot1")
        )
    )
))
# server.R
library(shiny)
library(UsingR)
data(galton)
shinyServer(function(input, output) {
        output$plot1 <- renderPlot({
            set.seed(2016-05-25)
            number_of_points <- input$numeric
            minX <- input$sliderX[1]
            maxX <- input$sliderX[2]
            minY <- input$sliderY[1]
            maxY <- input$sliderY[2]
            dataX <- runif(number_of_points, minX, maxX)
            dataY <- runif(number_of_points, minY, maxY)
            xlab <- ifelse(input$show_xlab, "X axis", "")
            ylab <- ifelse(input$show_ylab, "Y axis", "")
            main <- ifelse(input$show_title, "Title", "")
            plot(dataX, dataY, xlab=xlab, ylab=ylab, main=main,
                 xlim = c(-100,100), ylim=c(-100,100))
        })
    }
)

# Reactivity
# the reactive statements server.R functions seemingly follow different rules than ordinary R functions.
# Code in reactive functions of shinyServer get run repeatedly as needed when new values are entered. Reactive functions are those like renderPlot and renderPrint
# Example with prediction
# ui.R
library(shiny)
shinyUI(fluidPage(
    titlePanel("Predict Horsepower from MPG"),
    sidebarLayout(
        sidebarPanel(
            sliderInput("sliderMPG", "What is the MPG of the car?", 10, 35, value = 20),
            checkboxInput("showModel1", "Show/hide Model 1", value = TRUE),
            checkboxInput("showModel2", "Show/hide Model 2", value = TRUE),
            submitButton("Submit")
        ),
        mainPanel(
            plotOutput("plot1"),
            h3('Predicted Horsepower from Model 1:'),
            textOutput("pred1"),
            h3('Predicted Horsepower from Model 2:'),
            textOutput("pred2")
        )
    )
))
# server.R
library(shiny)
shinyServer(function(input, output) {
    mtcars$mpgsp <- ifelse(mtcars$mpg - 20 > 0, mtcars$mpg - 20, 0)
    model1 <- lm(hp ~ mpg, data = mtcars)
    model2 <- lm(hp ~ mpgsp + mpg, data = mtcars)
    
    model1pred <- reactive({
        mpgInput <- input$sliderMPG
        predict(model1, newdata = data.frame(mpg = mpgInput))
    })
    
    model2pred <- reactive({
        mpgInput <- input$sliderMPG
        predict(model2, newdata = data.frame(mpg = mpgInput,
                                             mpgsp = ifelse(mpgInput - 20 > 0,
                                                            mpgInput - 20, 0)))
    })
    
        output$plot1 <- renderPlot({
            mpgInput <- input$sliderMPG
            
            plot(mtcars$mpg, mtcars$hp, xlab = "Miles per gallon",
                 ylab = "Horsepower", bty = "n", pch = 16,
                 xlim = c(10, 35), ylim = c(50, 350))
            if(input$showModel1){
                abline(model1, col = "red", lwd = 2)
            }
            if(input$showModel2){
                model2lines <- predict(model2, newdata = data.frame(
                    mpg = 10:35, mpgsp = ifelse(10:35 - 20 > 0, 10:35 - 20, 0)
                ))
                lines(10:35, model2lines, col = "blue", lwd = 2)
            }
            legend(25, 250, c("Model 1 Prediction", "Model 2 Prediction"), pch = 16,
                   col = c("red", "blue"), bty = "n", cex = 1.2)
            points(mpgInput, model1pred(), col = "red", pch = 16, cex = 2)
            points(mpgInput, model2pred(), col = "blue", pch = 16, cex = 2)
        })
        output$pred1 <- renderText({
            model1pred()
        })
        output$pred2 <- renderText({
            model2pred()
        })
    }
)

# Advanced UI
# Examples to use tabs: tabsetPanel(), tabPanel()
# ui.R
library(shiny)
shinyUI(fluidPage(
    titlePanel("Tabs!"),
    sidebarLayout(
        sidebarPanel(
            textInput("box1", "Enter tab 1 text:", value = "Tab 1!"),
            textInput("box2", "Enter tab 2 text:", value = "Tab 2!"),
            textInput("box3", "Enter tab 3 text:", value = "Tab 3!")
        ),
        mainPanel(
            tabsetPanel(type = "tabs",
                        tabPanel("Tab 1", br(), textOutput("out1")),
                        tabPanel("Tab 2", br(), textOutput("out2")),
                        tabPanel("Tab 3", br(), textOutput("out3"))
                        )
        )
    )
))
# server.R
library(shiny)
shinyServer(function(input, output) {
    output$out1 <- renderText(input$box1)
    output$out2 <- renderText(input$box2)
    output$out3 <- renderText(input$box3)
    }
)

# 




```


