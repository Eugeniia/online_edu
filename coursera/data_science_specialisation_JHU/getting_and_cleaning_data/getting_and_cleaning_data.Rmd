---
title: "Getting and Cleaning Data"
author: "Evgeniia Golovina"
date: "24/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results="hide")
```

## Getting and Cleaning Data course (Coursera)

Week 1

```{r week_1, echo=FALSE}
setwd(paste0(getwd(), "/coursera/data_science_specialisation_JHU/getting_and_cleaning_data"))
file.exists("data") # check if the dir exists
dir.create("data") # create the data dir
if(!file.exists("data")){
  dir.create("data")
}
download.file() # get data from the internet

fileURL <- "https://www.smth"
download.file(fileURL, destfile = "./data/cameras.csv", method = "curl") # curl for https
list.files("./data")
dateDownloaded <- date(); dateDownloaded

library(xlsx) # to read xlsx files, XLConnect package
data <- read.xlsx("./data.xlsx", sheetIndex=1, header=TRUE); head(data)
data <- read.xlsx2("./data.xlsx", sheetIndex=1, header=TRUE); head(data) # much faster
data <- read.xlsx("./data.xlsx", sheetIndex=1, header=TRUE, colIndex=2:3, rowIndex=1:3)
write.xlsx()

# reading xml files
library(XML)
fileURL <- "http://smth"
doc <- xmlTreeParse(fileURL, useInternalNodes = TRUE) # htmlTreeParse for html files
rootNode <- xmlRoot(doc)
xmlName(rootNode); names(rootNode)
rootNode[[1]]; rootNode[[1]][[1]]
xmlSApply(rootNode, xmlValue) # programatically extract parts of the file
# XPath language
xpathSApply(rootNode, "//name", xmlValue)
xpathSApply(rootNode, "//price", xmlValue)

# reading json
library(jsonlite)
jsonData <- fromJSON("https://api.smth") # from API
names(jsonData); names(jsonData$owner)
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
iris2 <- fromJSON(myjson); head(iris2)

# data.table() 
library(data.table) # much faster than data.frame
df <- data.frame(x=rnorm(9), y=rep(c("a", "b", "c"), each=3), z=rnorm(9)); head(df, 3)
dt <- data.table(x=rnorm(9), y=rep(c("a", "b", "c"), each=3), z=rnorm(9)); head(df, 3)
tables() # see all the data tables in memory
dt[2,]; dt[dt$y=="a",]
dt[c(2,3)] # subsetting rows
dt[,c(2,3)] # subsetting columns?!? Nope
# expression - a collections of statements
{
  x = 1
  y = 2
}
k = {print(10);5} # 10
print(k) # 5
dt[, list(mean(x), sum(z))]
dt[,table(y)] # get y as a table
dt[,w:=z^2] # adding new columns
dt2 <- dt; dt[, y:=2]; head(dt, 3); head(dt2, 3) # use a copy function to copy the data table!
dt[, m:= {tmp <- (x+z); log2(tmp+5)}] # multiple operations
dt[, a:=x>0] # plyr like operations
dt[, b:= mean(x+w),by=a] # group by a

set.seed(123)
dt <- data.table(x=sample(letters[1:3], 1E5, TRUE))
dt[, .N, by=x] # special variables: .N - an integer of length 1 containing the number

dt <- data.table(x=rep(c("a", "b", "c"), each=100), y=rnorm(300))
setKey(dt, x); dt['a'] # Keys! to sort, subset much faster!
dt1 <- data.table(x=c("a", "b", "c", "dt1"), y = 1:4)
dt2 <- data.table(x=c("a", "b", "dt2"), z = 5:7)
setKey(dt1, x); setKey(dt2, x); merge(dt1, dt2) # the same key is y to mergy by x

big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names = FALSE, col.names = TRUE, sep = "\t", quote=FALSE)
system.time(fread(file))
system.time(read.table(file, header = TRUE, sep = "\t")) # slower
```

Week 1 Quiz

```{r week_1_quiz, echo=FALSE}
# Q1 - 53
setwd(paste0(getwd(), "/coursera/data_science_specialisation_JHU/getting_and_cleaning_data"))
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(fileURL, destfile = "./w1/data.csv", method = "curl")
dateDownloaded <- date(); dateDownloaded
data <- read.csv("./w1/data.csv"); head(data,3) # data$VAL == 24 to get property value >= $1,000,000
d <- data[data$VAL==24,]
t <- data$VAL; bad <- is.na(t); bad; m <- t[!bad]; length(m[m == 24])
# Q2 - Tidy data has one variable per column.
fesdf <- data[, c("FES", "VAL")]; data$FES
# Q3 - 36534720
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FDATA.gov_NGAP.xlsx"
download.file(fileURL, destfile = "./w1/data.xlsx", method = "curl")
dateDownloaded <- date(); dateDownloaded
install.packages("xlsx"); library(xlsx)
dat <- read.xlsx("./w1/data.xlsx", sheetIndex=1, header=TRUE, colIndex=7:15, rowIndex=18:23); dat
sum(dat$Zip*dat$Ext,na.rm=T) # 36534720
# Q4 - 127
install.packages("XML"); library(XML)
#fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Frestaurants.xml"
doc <- xmlTreeParse("./w1/getdata_data_restaurants.xml", useInternalNodes = TRUE)
rootNode <- xmlRoot(doc); zipcodes <- xpathSApply(rootNode, "//zipcode", xmlValue);
length(zipcodes[zipcodes=="21231"])
# Q5 - 
install.packages("data.table"); library(data.table)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w1/q5.csv", method = "curl")
dateDownloaded <- date(); dateDownloaded
DT <- fread("./w1/q5.csv"); v <- DT$pwgtp15; v
system.time(mean(DT[DT$SEX==1,]$pwgtp15)); system.time(mean(DT[DT$SEX==2,]$pwgtp15))
system.time(tapply(DT$pwgtp15,DT$SEX,mean)) # nope
system.time(sapply(split(DT$pwgtp15,DT$SEX),mean)) # nope
system.time(rowMeans(DT)[DT$SEX==1]); system.time(rowMeans(DT)[DT$SEX==2])
system.time(mean(DT$pwgtp15,by=DT$SEX)) 
system.time(DT[,mean(pwgtp15),by=SEX]) # yes
```

Week 2

```{r week_2, echo=FALSE}
setwd(paste0(getwd(), "/coursera/data_science_specialisation_JHU/getting_and_cleaning_data/w2"))
# Reading mySQL
install.packages("RMySQL"); library(RMySQL)
# to install mysql:
# cd /Library/LaunchDaemons
# sudo launchctl load -F com.oracle.oss.mysql.mysqld.plist # pass the macbook password
# About mysql shell - https://dev.mysql.com/doc/mysql-shell/8.0/en/mysql-shell-install.html
# https://genome.ucsc.edu/goldenPath/help/mysql.html
ucscDB <- dbConnect(MySQL(), user="genome", host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDB, "show databases;"); result; dbDisconnect(ucscDB)

hg19 <- dbConnect(MySQL(), user="genome", db="hg19", host="genome-mysql.cse.ucsc.edu")
all.tables <- dbListTables(hg19); length(all.tables); all.tables[1:5]
dbListFields(hg19, "affyU133Plus2") # get column names of the table
dbGetQuery(hg19, "select count(*) from affyU133Plus2") # get number of rows in the table
affyData <- dbReadTable(hg19, "affyU133Plus2"); head(affyData) # get as a dataframe
# select specific subset
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
affyMisSmall <- fetch(query, n=10); dbClearResult(query); dim(affyMisSmall)
dbDisconnect(hg19) # remember to close the connection!

# Reading from HDF5
#source("http://bioconductor.org/biocLite.R"); biocLite("rhdf5")
install.packages("BiocManager"); library(BiocManager); update.packages("BiocManager")
BiocManager::install(version = "3.12")
BiocManager::install(c("GenomicFeatures", "AnnotationDbi")) # example
BiocManager::install("rhdf5"); library(rhdf5)
created <- h5createFile("example.h5"); created
# create groups
created <- h5createGroup("example.h5", "foo")
created <- h5createGroup("example.h5", "baa")
created <- h5createGroup("example.h5", "foo/foobaa")
h5ls("example.h5")
# write to groups
A <- matrix(1:10, nr=5, nc=2); h5write(A, "example.h5", "foo/A")
B <- array(seq(0.1, 2.0, by=0.1), dim=c(5,2,2))
attr(B, "scale") <- "liter"; h5write(B, "example.h5", "foo/foobaa/B")
h5ls("example.h5")
# write a data set
df <- data.frame(1L:5L,seq(0,1,length.out = 5),
                 c("ab", "cde", "fdh", "a", "s"), stringsAsFactors = FALSE)
h5write(df, "example.h5", "df"); h5ls("example.h5")
# reading data
readA <- h5read("example.h5", "foo/A"); readA
readB <- h5read("example.h5", "foo/foobaa/B"); readB
readdf <- h5read("example.h5", "df"); readdf
# writing and reading chunks
h5write(c(12,13,14), "example.h5", "foo/A", index=list(1:3,1)) # writing 12, 13, 14 to the first three rows of the first column in foo/A
h5read("example.h5", "foo/A")

# Reading from the Web
con <- url("https://scholar.google.ru/citations?user=cR7t9_8AAAAJ&hl=en")
htmlCode <- readLines(con)
close(con); htmlCode

library(XML)
url <- "https://scholar.google.ru/citations?user=cR7t9_8AAAAJ&hl=en"
html <- xmlTreeParse(url, useInternalNodes = TRUE)
xpathSApply(html, "//title", xmlValue)

install.packages("httr"); library(httr)
html2 <- GET(url)
content2 <- content(html2, as="text")
parsedHTML <- htmlParse(content2, asText = T)
xpathSApply(parsedHTML, "//title", xmlValue)

pg2 <- GET("the url to password page", authenticate("user", "passswd")) # give username and password
pg2; names(pg2)
# using handles
google <- handle("https://www.google.ru")
pg1 <- GET(handle = google, path="/")
pg2 <- GET(handle = google, path="search")

# Reading from APIs
# accessing twitter from R
library(httr)
myapp <- oauth_app("twitter", key = "yourConsumerKeyHere", secret="yourConsumerSecretHere") # start the autorization process
sig <- sign_oauth1.0(myapp, token="yourTokenHere", token_secret = "yourTokenSecretHere")  # to sign in
homeTL <- GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig) # 1.1 is a version of the API; statuses - which data I'd like to get out
# converting the json object
json1 <- content(homeTL) # to extract the json data
json2 <- jsonlite::fromJSON(toJSON(json1)) # jsonlite package to reformat json as datafame
json2[1, 1:4]

# Reading from other sources
install.packages("foreign"); library(foreign) # read.foo where foo is a file extension
read.arff() # to read Weka
read.dta() # Stata
read.mtp() # Minitab
read.octave() # Octave
read.spss() # SPSS
read.xport() # SAS
# RPostresSQL, RODBC, RMongo, rmongodb packages
# reading images: jpeg, readbitmap, png and EBImage packages
# reading GIS data: rdgal, rgeos, raster packages
# reading music data: tuneR, seewave packages
```

Week 2 Quiz

```{r week_2_quiz, echo=FALSE}
# Q1 - 2013-11-07T13:25:07Z
install.packages("httpuv")
library(httr); library(httpuv)
oauth_endpoints("github")
myapp <- oauth_app("github", key = "987215e7eb60495356dc",
                   secret = "41b8888ac93af0fe65805862c116b2d75cb8027b") # to create a new OAuth app --> https://github.com/settings/developers, the URL is http://github.com and the Authorization callback URL is http://localhost:1410, specify key and secret
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp) # Get OAuth credentials
gtoken <- config(token = github_token) # Use API
req <- GET("https://api.github.com/users/jtleek/repos", gtoken)
stop_for_status(req)
cont <- content(req)
t <- c()
cont[[1]][["full_name"]]
for(i in 1:length(cont)){
  t <- c(t, cont[[i]][["full_name"]])
}
cont[[23]][["created_at"]]
# Q2 - sqldf("select pwgtp1 from acs where AGEP < 50")
setwd(paste0(getwd(), "/coursera/data_science_specialisation_JHU/getting_and_cleaning_data"))
install.packages("sqldf"); library(sqldf)
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w2/Q2.csv", method = "curl")
acs <- read.csv("./w2/Q2.csv"); head(acs,3)
sqldf("select * from acs") # nope
sqldf("select pwgtp1 from acs where AGEP < 50") # correct
sqldf("select pwgtp1 from acs") # nope
sqldf("select * from acs where AGEP < 50") # nope
# Q3 - sqldf("select distinct AGEP from acs")
unique(acs$AGEP)
sqldf("select unique * from acs") # nope, error
sqldf("select distinct pwgtp1 from acs") # nope
sqldf("select AGEP where unique from acs") # nope, error
sqldf("select distinct AGEP from acs") # yes
# Q4 - 45 31 7 25
library(XML); library(httr); library(httpuv)
url <- "http://biostat.jhsph.edu/~jleek/contact.html"
html <- GET(url)
content <- content(html, as="text")
parsedHTML <- htmlParse(content, asText = T)
# check the source code for the page
nchar('<meta name="Distribution" content="Global" />"') # 46 - 1
nchar('<script type="text/javascript">') # 31
nchar('  })();') # 7
nchar('				<ul class="sidemenu">') # 25

install.packages("RCurl"); library(RCurl)
surl <- "https://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
turl <- getURL(surl)
html <- htmlTreeParse(turl, useInternalNodes=T)
xpathSApply(html, "//td[@class='gsc_a_c']", xmlValue)
# Q5 - 32426.7
dd<-read.fwf("https://d396qusza40orc.cloudfront.net/getdata%2Fwksst8110.for", widths=c(10, rep(c(9,4),4)), skip=4)
sum(dd$V4) # 32426.7
```

Week 3

```{r week_3, echo=FALSE}
# Subsetting and sorting
set.seed(13435)
x <- data.frame("var1"=sample(1:5), "var2"=sample(6:10), "var3"=sample(11:15))
x <- x[sample(1:5),]; x; x$var2[c(1,3)] = NA; x
x[,1]; x[,"var1"]; x[1:2,"var2"]
x[(x$var1 <= 3 & x$var3 > 11),]; x[(x$var1 <= 3 | x$var3 > 15),]
x[which(x$var2 > 8),] # dealing with missing values
# sorting
sort(x$var1); sort(x$var1, decreasing = T); sort(x$var2, na.last = T) # add NAs in the end of the sort
# ordering
x[order(x$var1),] # order by var1
x[order(x$var1, x$var3),] # order by var1 abd then by var3
library(plyr); arrange(x, var1); arrange(x, desc(var1)) # ordering with plyr
x$var4 <- rnorm(5); x # adding rows and columns
y <- cbind(x, rnorm(5)); y; y <- cbind(rnorm(5), x); y # column-binding
z <- rbind(x, rnorm(4)); z # row-binding

# Summarizing data
setwd(paste0(getwd(), "/coursera/data_science_specialisation_JHU/getting_and_cleaning_data"))
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w2/Q2.csv", method = "curl")
acs <- read.csv("./w2/Q2.csv"); head(acs,3); tail(acs,4) # default is 6
summary(acs)
str(acs)
quantile(acs$pwgtp80, na.rm = T); quantile(acs$pwgtp80, na.rm = T, probs = c(0.5, 0.75, 0.9))
table(acs$pwgtp80, useNA = "ifany") # make a table
table(acs$pwgtp80, acs$pwgtp79) # make a 2-dimensional table
sum(is.na(acs$pwgtp80)) # check for missing values
any(is.na(acs$pwgtp80)) # FALSE
all(acs$pwgtp80 > 0) # TRUE - all have values more than 0
colSums(is.na(acs)); rowSums(is.na(acs))
all(colSums(is.na(acs))==0) # FALSE
table(acs$SERIALNO %in% c(85383)); table(acs$SERIALNO %in% c(85383, 91107)) # values with specific characteristics
acs[acs$SERIALNO %in% c(85383, 91107),] # to get a subset with specific values
# cross tabs
data("UCBAdmissions")
df <- as.data.frame(UCBAdmissions); summary(df)
xt <- xtabs(Freq ~ Gender + Admit, data=df); xt # get Freq info for Geneder across Admit
# flat tables
warpbreaks$replicate <- rep(1:9, len=54)
xt <- xtabs(breaks ~ ., data=warpbreaks); xt # . means break it down across all variables
ftable(xt) # to make a flat table
# size of the dataset
fakedata <- rnorm(1e5); object.size(fakedata)
print(object.size(fakedata), units="Mb")

# Creating new variables
setwd(paste0(getwd(), "/coursera/data_science_specialisation_JHU/getting_and_cleaning_data"))
fileURL <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
download.file(fileURL, destfile = "./w2/Q2.csv", method = "curl")
acs <- read.csv("./w2/Q2.csv"); head(acs,3)
# creating sequences - to index your data set
s1 <- seq(1,10, by=2); s1 # increasing by 2
s2 <- seq(1,10, length=3); s2 # create exact three values
x <- c(1, 3, 8, 25, 100); seq(along=x) # create an index for x
acs$new <- acs$SERIALNO %in% c(85383, 91107); table(acs$new)
acs$newpw <- ifelse(acs$pwgtp80 > 20, TRUE, FALSE); table(acs$newpw, acs$pwgtp80 > 20) # binary vars
acs$newgr <- cut(acs$pwgtp80, breaks = quantile(acs$pwgtp80)); table(acs$newgr) # categorical vars
table(acs$newgr, acs$pwgtp80)
# easier cutting
install.packages("Hmisc"); library(Hmisc)
acs$newgr2 <- cut2(acs$pwgtp80, g=4); table(acs$newgr) # break into four groups according to quantiles
acs$zcf <- factor(acs$SERIALNO); acs$zcf[1:10]; class(acs$zcf) # create factor vars
# levels of factor variables
yesno <- sample(c("yes", "no"), size=10, replace=T); yesno
yesnofac <- factor(yesno, levels=c("yes", "no")); yesnofac
relevel(yesnofac, ref="yes")
as.numeric(yesnofac)
# using the mutate function
library(Hmisc); library(plyr)
acs.new <- mutate(acs, new_pwgtp80=cut2(pwgtp80,g=4)); table(acs.new$new_pwgtp80) # create a new version of a variable and simultaneously add it to a dataset
# Common transforms
abs(-2) # absolute value
sqrt(2)
ceiling(3.475) # 4
floor(3.475) # 3
round(3.475, digits=2) # 3.48
signif(3.475, digits=2) # 3.5
cos(2); sin(2)
log(2); log2(2); log10(2)
exp(2)

# Reshaping data
library(reshape2); head(mtcars)
# melting data frames
mtcars$carname <- rownames(mtcars)
carMelt <- melt(mtcars, id=c("carname", "gear", "cyl"), measure.vars = c("mpg", "hp")) # id vaiables and measure variables
head(carMelt, 3); tail(carMelt, 3) # in variable column you see both mpg and hp; in value column you see the values for mpg and hp variables
# casting data frames
cylData <- dcast(carMelt, cyl ~ variable); cylData # reformatting the dataset: see the cyl breaken down by different variables; summarizes the dataset by length: for cyl "4" we have 11 measures of mpg and 11 measures of hp
cylData <- dcast(carMelt, cyl ~ variable, mean); cylData # take mean for each value
# averaging values
head(InsectSprays)
tapply(InsectSprays$count, InsectSprays$spray, sum) # apply to count along the index spray the sum fun
spIns <- split(InsectSprays$count, InsectSprays$spray); spIns # split counts by different spray
sprCount <- lapply(spIns, sum); sprCount # for each list element we apply sum
unlist(sprCount) # combine, convert list to vector
sapply(spIns, sum) # the same
ddply(InsectSprays,.(spray), summarise, sum=sum(count)) # spray to summarise
spraySums <- ddply(InsectSprays,.(spray), summarise, sum=ave(count, FUN=sum)); dim(spraySums)
head(spraySums) # apply the sum function to each variable in the original dataset
acast() # to convert to array
arrange()

# Managing data frames with dplyr
library(dplyr)
options(width = 105)
head(mtcars); dim(mtcars); str(mtcars)
names(mtcars)
# select
head(select(mtcars, disp:vs)) # select all columns between disp and vs
head(select(mtcars, -(disp:vs))) # select all columns except those between disp and vs
i <- match("disp", names(mtcars)); j <- match("vs", names(mtcars)); head(mtcars[, -(i:j)]) # the same
# filter
mt <- filter(mtcars, mpg > 20); mt
mt <- filter(mtcars, mpg > 20 & disp < 100); mt
# arrange - reorder df rows based on the values of df columns
c <- arrange(mtcars, hp); c
c <- arrange(mtcars, desc(hp)); c
# rename
mt <- rename(mtcars, cylinder = cyl, horsepower = hp); mt
# mutate - to transform the existing variable or create the new ones
mt <- mutate(mtcars, mpgtrend = mpg-mean(mpg, na.rm=T)); head(select(mt, mpg, mpgtrend))
# groupby - split a dataframe according to certain categorical variables
mt <- mutate(mtcars, newcyl = factor(1 * (cyl > 4), labels = c("low", "high"))); mt
highcyl <- group_by(mt, newcyl); highcyl
# summarize
summarize(highcyl, mpgn = mean(mpg, na.rm = T), hpn = max(hp), wtn = median(wt))
# %>% operator to chain different operations together, so basically you don't need to specify the dataframe of the previous operation or output
mtcars %>% mutate(newcyl = factor(1 * (cyl > 4), labels = c("low", "high"))) %>% group_by(newcyl) %>% summarize(mpgn = mean(mpg, na.rm = T), hpn = max(hp), wtn = median(wt))

# Merging data
setwd(getwd(), "/coursera/data_science_specialisation_JHU/getting_and_cleaning_data"))
url1 <- "https://dl.dropboxusercontent.com/u/7710864/data/reviews-apr29.csv"
url2 <- "https://dl.dropboxusercontent.com/u/7710864/data/solutions-apr29.csv"
download.file(url1, destfile = "./w3/reviews.csv", method = "curl")
download.file(url2, destfile = "./w3/solutions.csv", method = "curl")
reviews <- read.csv("./w3/reviews.csv"); solutions <- read.csv("./w3/solutions.csv")
# merge() x,y - dataframes; by, by.x, by.y - by which column to merge; by default it will merge by all columns that have the same name in both dataframes (by); all = T include rows with missing values
mergeData <- merge(reviews, solutions, by.x="solution_id", by.y="id", all=TRUE) # merge dataframes by solution_id in reviews and by id in solutions dataframe
intersect(names(solutions), names(reviews))
mergeData2 <- merge(reviews, solutions, all=TRUE) # merge by all common column names
# join
df1 <- data.frame(id=sample(1:10), x=rnorm(10))
df2 <- data.frame(id=sample(1:10), y=rnorm(10))
arrange(join(df1, df2), id)
# join multiple dataframes
df1 <- data.frame(id=sample(1:10), x=rnorm(10))
df2 <- data.frame(id=sample(1:10), y=rnorm(10))
df3 <- data.frame(id=sample(1:10), z=rnorm(10))
dfList <- list(df1, df2, df3)
join_all(dfList)
```

Week 3 Quiz

```{r week_3_quiz, echo=FALSE}
# Q1 - 125, 238, 262
url1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(url1, destfile = "./w3/Q1.csv", method = "curl")
df <- read.csv("./w3/Q1.csv")
df$ACR # == 3
df$AGS # == 6
#agricultureLogical <- filter(df, ACR == 3 & AGS == 6)
agricultureLogical <- (df$ACR == 3 & df$AGS == 6)
which(agricultureLogical) # 125, 238, 262
# Q2 - -15259150 -10575416 
library(httr); library(jpeg)
GET("https://d396qusza40orc.cloudfront.net/getdata%2Fjeff.jpg", write_disk("./w3/Q2.jpg"))
j <- readJPEG("./w3/Q2.jpg", native=T)
quantile(j, na.rm = T, probs = c(0.3, 0.8)) # -15259150 -10575416 
# Q3 - 189 matches, 13th country is St. Kitts and Nevis
gdp_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
edu_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(gdp_url, destfile = "./w3/Q3_gdp.csv", method = "curl")
download.file(edu_url, destfile = "./w3/Q3_edu.csv", method = "curl")
gdp <- read.csv("./w3/Q3_gdp.csv")
edu <- read.csv("./w3/Q3_edu.csv")
head(gdp,3); head(edu,3)
# GDP cleaning
k <- gdp %>% select(c(X, Gross.domestic.product.2012, X.2)) %>% rename(CountryCode = X, Ranking = Gross.domestic.product.2012, Economy=X.2)
k1 <- k[which(k$CountryCode > 0 & k$Ranking > 0),]
k2 <- arrange(join(edu, k1), CountryCode)
k2 <- merge(edu, k1) # 189
k2 <- merge(k1, edu) # 189
k2$Ranking <- as.numeric(k2$Ranking)
k3 <- k2[order(k2$Ranking,decreasing=TRUE),]; k3[13, "Economy"]
# Q4 - 32.96667, 91.91304
names(c_des)
c_des$Income.Group
hi_OECD <- filter(c_des, Income.Group=="High income: OECD"); hi_OECD
hi_nonOECD <- filter(c_des, Income.Group=="High income: nonOECD"); hi_nonOECD
av_OECD <- mean(hi_OECD$Ranking); av_OECD
av_nonOECD <- mean(hi_nonOECD$Ranking); av_nonOECD
# Q5 - 5
library(Hmisc)
c_des$newgr <- cut2(c_des$Ranking, g=5); table(c_des$newgr, c_des$Income.Group) # 5
```

Week 4

```{r week_4, echo=FALSE}
# Editing text variables
setwd(paste0(getwd(), "/coursera/data_science_specialisation_JHU/getting_and_cleaning_data"))
edu_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(edu_url, destfile = "./w3/Q3_edu.csv", method = "curl")
edu <- read.csv("./w3/Q3_edu.csv"); head(edu,3)
names(edu); tolower(names(edu)); toupper(names(edu)) # make all the letters lowercase
# fixing character vectors - strsplit()
splitNames <- strsplit(names(edu), "\\."); splitNames[[2]] # split by period, use \\ escape character because period is reserved
# quick aside - lists
mylist <- list(letters = c("A", "b", "c"), numbers = 1:3, matrix(1:25, ncol=5)); head(mylist)
mylist[1]; mylist$letters; mylist[[1]]
splitNames[[2]][1] # split and take the first element
firstElement <- function(x) {x[1]}
sapply(splitNames, firstElement)
# fixing character vectors - sub(), gsub()
edu$new_var <- rnorm(234); names(edu); sub("_", "", names(edu),); sub("\\.", "", names(edu),)
test <- "this_is_test"; sub("_", "", test); gsub("_", "", test); gsub("\\.", "", names(edu),)
# finding values - grep(), grepl()
grep("System", gsub("\\.", "", names(edu),)) # return the index of the found variable
grep("Republic", edu$Long.Name)
table(grepl("System", gsub("\\.", "", names(edu),))) # return number of variables having the pattern
newt <- edu[!grepl("System", gsub("\\.", "", names(edu),)),] # return only those rows that don't have System in the value
grep("System", gsub("\\.", "", names(edu),), value=T)
grep("Lol", edu$Long.Name); length(grep("Lol", edu$Long.Name)) # a way to check if the value exist
# stringr package
install.packages("stringr"); library(stringr)
nchar("System"); substr("System", 1, 3) # to find the first through the third letters
paste("Sys", "tem"); paste0("Sys", "tem") # paste without space
strtrim("System   ", 5)

# Regular expressions
# Working with dates
d1 <- date(); d1; class(d1)
d2 <- Sys.Date(); d2; class(d2)
format(d2, "%a %b %d")
x <- c("1jan1960", "2jan1960", "31mar1960", "30jul1960"); z <- as.Date(x, "%d%b%Y"); z
z[2] - z[1]; as.numeric(z[2] - z[1])
# converting to julian
weekdays(d2); months(d2); julian(d2)
# lubridate package
install.packages("lubridate"); library(lubridate)
ymd("20140108"); mdy("08/04/2013"); dmy("03-04-2013") # convert number to a date
ymd_hms("2011-02-10 10:15:03")
ymd_hms("2011-02-10 10:15:03", tz="Pacific/Auckland")
?Sys.timezone
x <- dmy(c("1jan1960", "2jan1960", "31mar1960", "30jul1960"))
wday(x[1]); wday(x[1], label = T)
```

Week 4 Quiz

```{r week_4_quiz, echo=FALSE}
# Week 4 Quiz
# Q1 - "" "15"
url1 <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06hid.csv"
download.file(url1, destfile = "./w4/Q1.csv", method = "curl")
acs <- read.csv("./w4/Q1.csv")
names(acs)
splitNames <- strsplit(names(acs), "wgtp"); splitNames[123] # "" "15"
# Q2 - 377652.4
gdp_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
download.file(gdp_url, destfile = "./w4/Q2_gdp.csv", method = "curl")
gdp <- read.csv("./w4/Q2_gdp.csv")
head(gdp,3)
# GDP cleaning
library(dplyr)
k <- gdp %>% select(c(X, Gross.domestic.product.2012, X.2, X.3)) %>% rename(CountryCode = X, Ranking = Gross.domestic.product.2012, Economy=X.2, Money=X.3)
k1 <- k[which(k$CountryCode > 0 & k$Ranking > 0),]
k1$Money
k1$Money_new <- as.numeric(gsub(",", "", k1$Money))
mean(k1$Money_new, na.rm=T) # 377652.4
# Q3 - grep("^United",countryNames), 3
# to count the number of countries whose name begins with "United", should be 3
grep("*United",k1$Economy) # nope
grep("^United",k1$Economy) # this one returns indeces for three found variables
grep("*United",k1$Economy) # nope
grep("United$",k1$Economy) # nope
# Q4 - 13
gdp_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FGDP.csv"
edu_url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2FEDSTATS_Country.csv"
download.file(gdp_url, destfile = "./w4/Q4_gdp.csv", method = "curl")
download.file(edu_url, destfile = "./w4/Q4_edu.csv", method = "curl")
gdp <- read.csv("./w4/Q4_gdp.csv")
edu <- read.csv("./w4/Q4_edu.csv")
head(gdp,3); head(edu,3)
# GDP cleaning
k <- gdp %>% select(c(X, Gross.domestic.product.2012, X.2)) %>% rename(CountryCode = X, Ranking = Gross.domestic.product.2012, Economy=X.2)
k1 <- k[which(k$CountryCode > 0 & k$Ranking > 0),]
k2 <- merge(edu, k1) # 189
k2 <- merge(k1, edu) # 189
#k2$Ranking <- as.numeric(k2$Ranking)
#k3 <- k2[order(k2$Ranking,decreasing=TRUE),]; k3[13, "Economy"]
names(k2)
fyear <- k2$Special.Notes
grep("*(Fiscal year end: June)",k2$Special.Notes) # 13
# Q5 - 47
install.packages("quantmod"); library(quantmod)
amzn = getSymbols("AMZN",auto.assign=FALSE)
sampleTimes = index(amzn)
length(grep("*(2012)", sampleTimes)) # 250
sampleTimes2012 <- sampleTimes[grep("*(2012)", sampleTimes)]
# Monday is 2
co <- 0
for(i in 1:length(sampleTimes2012)){
  if(wday(sampleTimes2012[i], label = T)=="Mon"){
    print("This is Monday ")
    co <- co + 1
  }
}
co # 47
```
