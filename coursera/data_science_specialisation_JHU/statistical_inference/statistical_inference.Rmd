---
title: "Statistical Inference"
author: "Evgeniia Golovina"
date: "24/02/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, results="hide")
```

## Statistical Inference course (Coursera)

Week 1 Quiz

```{r week_1_quiz}
# Q1 - 11%
# Q2 - 0.75
# Q3 - p/(1-p) = Y/X
# Q4 - The median must be 0
# Q5 - 3
# Q6 - 40%
```

Week 2 Quiz

```{r week_2_quiz}
# Q1 - Ïƒ^2/n
# Q2 - How many sds below the mean 70 is? (70-80)/10 = -1;pnorm(-1) = ~16%
# Q3 - qnorm(.95, mean = 1100, sd = 75) = ~1223
# Q4 - qnorm(0.95, mean = 1100, sd = 75/sqrt(100)) ~1112
# Q5 - pbinom(3, size = 5, prob = 0.5, lower.tail = FALSE) 0.1875
# Q6 - pnorm(16, mean = 15, sd = 1) - pnorm(14, mean = 15, sd = 1) 0.6827
# Q7 - According to the LLN it should be near .5
# Q8 - ppois(10, lambda = 5*3) 0.1185
```

Week 3

```{r w3}
# comparing the t and Z distributions
library(manipulate)
k <- 1000
xvals <- seq(-5, 5, length = k)
myplot <- function(df){
  d <- data.frame(y = c(dnorm(xvals), dt(xvals, df)),
                  x = xvals,
                  dist = factor(rep(c("Normal", "T"), c(k,k))))
  g <- ggplot(d, aes(x = x, y = y))
  g <- g + geom_line(size = 2, aes(color = dist))
  g
}
manipulate(myplot(mu), mu = slider(1, 20, step = 1)) # t distribution has thicker or heavier tails

# comparing the upper quantiles of the t and Z densities
pvals <- seq(.5, .99, by = .01)
myplot2 <- function(df){
  d <- data.frame(n= qnorm(pvals),t=qt(pvals, df),
                  p = pvals)
  g <- ggplot(d, aes(x= n, y = t))
  g <- g + geom_abline(size = 2, col = "lightblue")
  g <- g + geom_line(size = 2, col = "black")
  g <- g + geom_vline(xintercept = qnorm(0.975))
  g <- g + geom_hline(yintercept = qt(0.975, df))
  g
}
manipulate(myplot2(df), df = slider(1, 20, step = 1)) # t intervals will be always wider that Z intervals

# t confidence intervals example
data(sleep); head(sleep)
# paired observations are connected with a line; comparing subject specific differences when comparing acros the groups not the variations bwteen groups
g1 <- sleep$extra[1:10]; g2 <- sleep$extra[11:20]
difference <- g2 - g1
mn <- mean(difference); s <- sd(difference); n <- 10
# calculating 95% t confidence interval directly
mn + c(-1, 1) * qt(.975, n-1)*s/sqrt(n)
# using R's built in function
t.test(difference)
# using R's built in function, another format
t.test(g2, g1, paired = TRUE)
# using R's built in function, another format
t.test(extra ~ I(relevel(group, 2)), paired = TRUE, data = sleep)

# estimating pooled variance: comparing SBP for 8 oral contraceptive users (X) versus 21 controls (Y); mean of X is 132.86, sd of X is 15.34; mean of Y is 127.44, sd of Y is 18.23
sp <- sqrt((7*15.34^2 + 20 * 18.23^2)/(8 + 21 - 2))
132.86 - 127.44 + c(-1,1) * qt(.975, 27) * sp * (1 / 8 + 1 / 20)^.5 # independent group t interval

# mistakenly treating the sleep data as grouped: different intervals for grouped vs paired! Because intersubject variability exusts!
n1 <- length(g1); n2 <- length(g2)
sp <- sqrt( ((n1 - 1) * sd(x1)^2 + (n2-1) * sd(x2)^2) / (n1 + n2-2))
md <- mean(g2) - mean(g1)
semd <- sp * sqrt(1 / n1 + 1/n2)
rbind(md + c(-1, 1) * qt(.975, n1 + n2 - 2) * semd,
      t.test(g2, g1, paired = FALSE, var.equal = TRUE)$conf,
      t.test(g2, g1, paired = TRUE)$conf
)

# ChickWeight example
library(datasets); data(ChickWeight); library(reshape2); library(dplyr)
# defining weight gain or loss
wideCW <- dcast(ChickWeight, Diet + Chick ~ Time, value.var = "weight")
names(wideCW)[-(1 : 2)] <- paste("time", names(wideCW)[-(1 : 2)], sep = "")
wideCW <- mutate(wideCW, gain = time21 - time0)
# calculating t intervals
wideCW14 <- subset(wideCW, Diet %in% c(1, 4))
rbind(t.test(gain ~ Diet, paired = FALSE, var.equal = TRUE, data = wideCW14)$conf,
      t.test(gain ~ Diet, paired = FALSE, var.equal = FALSE, data = wideCW14)$conf
)

# t test
library(UsingR); data(father.son)
t.test(father.son$sheight - father.son$fheight)
# One Sample t-test
#data: father.son$sheight - father.son$fheight
#t = 11.79, df = 1077, p-value < 2.2e-16
#alternative hypothesis: true mean is not equal to 0
#95 percent confidence interval:
#0.831 1.163
#sample estimates:
#mean of x
#0.997

# two group confidence intervals
library(datasets); data(ChickWeight); library(reshape2); library(dplyr)
# defining weight gain or loss
wideCW <- dcast(ChickWeight, Diet + Chick ~ Time, value.var = "weight")
names(wideCW)[-(1:2)] <- paste("time", names(wideCW)[-(1:2)], sep = "")
wideCW <- mutate(wideCW, gain = time21 - time0)
# Two sample t-test. Unequal variance T test comparing diets 1 and 4.
wideCW14 <- subset(wideCW, Diet %in% c(1, 4))
t.test(gain ~ Diet, paired = FALSE, var.equal = TRUE, data = wideCW14)
# t = -2.7252 is an estimate that the difference in the average weight gain between the two diets less then hypethisied value 0. t statistics calculates how many standard estimated errors our difference in means is from hypothesised

# p values
pt(2.5, 15, lower.tail = FALSE)
# if each gender has an independent 50% probability for each birth, what's the probability of gettting 7 or more girls out of 8 births?
choose(8,7) * 0.5^8 + choose(8,8) * 0.5^8 # binomial calculations
pbinom(6, size = 8, prob = 0.5, lower.tail = FALSE)
```

Week 3 Homework and Quiz

```{r w3_homework_and_quiz}
## Homework
# Q1
t.test(mtcars$mpg) # 95 percent confidence interval: 17.91768 22.26357
# Q2
n = 9; sd = 1
qt(.975, 8)
y <- qt(.975, 8) * sd/sqrt(9) # 0.768668 for lower endpoint
# Q3 - should be statistically independent
# Q4
head(mtcars)
c4 <- mtcars$mpg[mtcars$cyl==4]; c6 <- mtcars$mpg[mtcars$cyl==6]
t.test(c4, c6, var.equal = TRUE) # 3.154286 10.687272
# Q5 - wide interval is better
# Q6 - the interval is above 0 and 4 is better than 6 in the terms of MPG
# Q7 - 
n1 <- 9; n2 <- 9
sd1 <- 1.5; sd2 <- 1.8
sp <- ((n1-1) * sd1^2 + (n2-1) * sd2^2) / (n1 + n2-2) # 2.745
# Q8 - the proportion of successes
# Q9 - assigning a prior probability distribution

## Quiz
# Q1 - 1077, 1123
qt(.975, 8) # 1.859548
sd <- 30
semd <- sd/sqrt(9) 
1100 + c(-1, 1) * qt(.975, 8) * semd # 1076.94 1123.06
# Q2 - 2.60
y <- -2; n = 9; qt(.975, 8)
qt(.975, 8) * sd/3 = -2; sd <- abs(-6/qt(.975, 8)) # 2.60
# Q3 - a paired interval
# Q4 - -2.751649 -1.248351
# interval for new system
m_new <- 3
sd_new <- sqrt(0.60); s_new = 0.60
semd_new <- sd_new/sqrt(10) 
m_new + c(-1, 1) * qt(.975, 9) * semd_new
# interval for old system
m_old <- 5
sd_old <- sqrt(0.68); s_old <- 0.68
semd_old <- sd_old/sqrt(10) 
m_old + c(-1, 1) * qt(.975, 9) * semd_old
# interval for both
n1 <- 10; n2 <- 10
sp <- sqrt(((n1 - 1) * s_new + (n2-1) * s_old) / (n1 + n2-2))
md <- m_new - m_old
semd <- sp * sqrt(1/n1 + 1/n2)
md + c(-1, 1) * qt(.975, n1 + n2 - 2) * semd # -2.751649 -1.248351
# Q5 - the interval will be narrower
# Q6 - interval is above zero; the new system appears to be effective
# substracting: new - old
n1 <- n2 <- 100; m_new <- 4; m_old <- 6; sd_new <- 0.5; sd_old <- 2
sp <- sqrt(((n1-1) * sd_new^2 + (n2-1) * sd_old^2) / (n1 + n2-2))
md <- m_new - m_old
semd <- sp * sqrt(1/n1 + 1/n2)
md + c(-1, 1) * qt(.975, n1 + n2 - 2) * semd
# calculating degrees of freedom
df <- ((sd_new^4/n1 + sd_old^4/n2)^2)/((((sd_new^4/n1)^2)/(n1-1))+(((sd_old^4/n2)^2)/(n2-1)))
md + c(-1, 1) * qt(.975, df) * semd # -2.409018 -1.590982
# substracting: old - new
n_new <- n_old <- 100; m_new <- 4; m_old <- 6; sd_new <- 0.5; sd_old <- 2
sp <- sqrt(((n_old-1) * sd_old^2 + (n_new-1) * sd_new^2) / (n_old + n_new-2))
md <- m_old - m_new
semd <- sp * sqrt(1/n_old + 1/n_new)
md + c(-1, 1) * qt(.975, n_old + n_new - 2) * semd
# calculating degrees of freedom
df <- ((sd_old^4/n_old + sd_new^4/n_new)^2)/((((sd_old^4/n_old)^2)/(n_old-1))+(((sd_new^4/n_new)^2)/(n_new-1)))
md + c(-1, 1) * qt(.975, df) * semd # 1.590982 2.409018
# Q7 - -5.364, -2,636
n_tr <- n_pl <- 9; m_tr <- -3; m_pl <- 1; sd_tr <- 1.5; sd_pl <- 1.8
sp <- sqrt(((n_tr-1) * sd_tr^2 + (n_pl-1) * sd_pl^2) / (n_tr + n_pl-2))
md <- m_tr - m_pl
semd <- sp * sqrt(1/n_tr + 1/n_pl)
df <- ((sd_tr^4/n_tr + sd_pl^4/n_pl)^2)/((((sd_tr^4/n_tr)^2)/(n_tr-1))+(((sd_pl^4/n_pl)^2)/(n_pl-1)))
md + c(-1, 1) * qt(.95, df) * semd # -5.373856 -2.626144
md + c(-1, 1) * qt(.95, 16) * semd # -5.363579 -2.636421
```

Week 4

```{r w4}
# calculating power for Gaussian dat
alpha = 0.05; z <- qnorm(1 - alpha)
pnorm(mu0 + z*sigma/sqrt(n), mean=mua, sd=sigma/sqrt(n), lower.tail = FALSE)

# example
mu0 = 30; mua = 32; sigma = 4; n = 16; alpha = 0.05
z <- qnorm(1 - alpha)
pnorm(mu0 + z*sigma/sqrt(n), mean=mua, sd=sigma/sqrt(n), lower.tail = FALSE)

# investigating power
library(manipulate)
mu0 = 30
myplot <- function(sigma, mua, n, alpha) {
  g = ggplot(data.frame(mu = c(27, 36)), aes(x = mu))
  g = g + stat_function(fun = dnorm, geom = "line", args = list(mean = mu0,
                                                                sd = sigma/sqrt(n)), size = 2, 
                        col = "red")
  g = g + stat_function(fun = dnorm, geom = "line", args = list(mean = mua,
                                                                sd = sigma/sqrt(n)), size = 2, 
                        col = "blue")
  xitc = mu0 + qnorm(1 - alpha) * sigma/sqrt(n)
  g = g + geom_vline(xintercept = xitc, size = 3)
  g
}
manipulate(myplot(sigma, mua, n, alpha), sigma = slider(1, 10, step = 1, initial= 4),
           mua = slider(30, 35, step = 1, initial = 32), n = slider(1, 50, step = 1, initial = 16), 
           alpha = slider(0.01, 0.1, step = 0.01, initial = 0.05))

# T test power; delta is the difference in the means
# omitting the power and getting a power estimate
power.t.test(n = 16, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$power
# illustrating that it depends only on the effect size which is delta/sd
power.t.test(n = 16, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$power
# same thing again
power.t.test(n = 16, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$power
# specifying the power and getting n
power.t.test(power = 0.8, delta = 2/4, sd = 1, type = "one.sample", alt = "one.sided")$n
# again illustrating that the effect size is all that matters
power.t.test(power = 0.8, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$n
# again illustrating that the effect size is all that matters
power.t.test(power = 0.8, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$n
# specifying the power and n, and getting delta
power.t.test(power = 0.8, n = 100, sd = 1, type = "one.sample", alt = "one.sided")$delta

# Multiple testing
# case study 1: no true positives
set.seed(1234)
pValues <- rep(NA, 1000)
for(i in 1:1000){
  y <- rnorm(20)
  x <- rnorm(20)
  pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
}
sum(pValues < 0.05)
# controlling FWER
sum(p.adjust(pValues, method="bonferroni") < 0.05)
# controlling FDR
sum(p.adjust(pValues, method="BH") < 0.05)

# case study 2: 50% true positives
set.seed(1234)
pValues <- rep(NA, 1000)
for(i in 1:1000){
  y <- rnorm(20)
  if(i <= 500){y <- rnorm(20)}else{y <- rnorm(20, mean=2*x)}
  pValues[i] <- summary(lm(y ~ x))$coeff[2,4]
}
trueStatus <- rep(c("zero", "not_zero"), each=500)
table(pValues < 0.05, trueStatus)
# controlling FWER
sum(p.adjust(pValues, method="bonferroni") < 0.05) # 500 true false positives
table(p.adjust(pValues, method="bonferroni") < 0.05, trueStatus)
# controlling FDR
sum(p.adjust(pValues, method="BH") < 0.05) # 9 extra false positives!
table(p.adjust(pValues, method="BH") < 0.05, trueStatus)

# p-values vs adjusted p-values
par(mfrow=c(1,2))
plot(pValues, p.adjust(pValues, method="bonferroni"), pch=19)
plot(pValues, p.adjust(pValues, method="BH"), pch=19)

# resampling
# bootstrapping
library(UsingR); data(father.son)
x <- father.son$sheight
n <- length(x)
B <- 10000
resamples <- matrix(sample(x, n * B, replace = TRUE), B, n)
resampledMedians <- apply(resamples, 1, median)
# nonparametric bootstrapping
B <- 10000
resamples <- matrix(sample(x, n * B, replace = TRUE), B, n)
medians <- apply(resamples, 1, median)
sd(medians) # calculating stadnard deviations of the medians to estimate standard error of median
quantile(medians, c(.025, .975)) # # percentiles as a confidence interval for median
g = ggplot(data.frame(medians = medians), aes(x = medians))
g = g + geom_histogram(color = "black", fill = "lightblue", binwidth = 0.05)
g

# permutation test
data(InsectSprays)
g = ggplot(InsectSprays, aes(spray, count, fill = spray))
g = g + geom_boxplot()
g
# permutation for groups B and C
subdata <- InsectSprays[InsectSprays$spray %in% c("B", "C"),]
y <- subdata$count
group <- as.character(subdata$spray)
testStat <- function(w, g) mean(w[g=="B"]) - mean(w[g=="C"])
observedStat <- testStat(y, group)
permutations <- sapply(1:10000, function(i) testStat(y, sample(group)))
observedStat
# proportion of times we got a simulated statistic larger than our observed statistic
mean(permutations > observedStat)
```

Week 4 Homework and Quiz

```{r w4_homework_and_quiz}
## Homework
# Q1 - 


# Q2 - 


## Quiz
# Q1 - 

# Q2 - 

```


